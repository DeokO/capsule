\input{preamble/preamble.tex}
\input{preamble/preamble_math.tex}
\input{preamble/preamble_acronyms.tex}

% fanciness for quote
\usepackage{epigraph}
\patchcmd{\epigraph}{\@epitext{#1}}{\itshape\@epitext{#1}}{}{}

\begin{document}

\title{Who, What, When, Where, and Why? \\ A Computational Approach to Understanding Historical Events Using State Department Cables}

\author{Allison J.B. Chaney, Hanna Wallach, David M. Blei}

\maketitle

\vspace{0.3in}

\begin{center}
\parbox{3.5in}{
\raggedright{
 	\emph{We can do nothing but scrutinize historical events themselves if we want to discover what they are.}}\\
\smallskip
\raggedleft{-- Dean W.R. Matthews, \emph{What is an Historical Event?}}
}
\end{center}

\vspace{0.5in}

\begin{abstract}
	We develop computational methods for analyzing historical documents to identify events of potential historical significance. Significant events are characterized by interactions between entities (e.g., countries, organizations, individuals) that deviate from typical interaction patterns. When studying historical events, historians and political scientists commonly read large quantities of text to construct an accurate picture of who, what, when, and where---a necessary precursor to answering the more nuanced question, ``Why?'' Our methods help historians identify possible events from the texts of historical communication. Specifically, we build on topic modeling to distinguish between topics that describe ``business-as-usual'' and topics that deviate from these patterns, where deviations are also indicated by particular entities interacting during particular periods of time. To demonstrate our methods, we analyze a corpus of 2 million State Department cables from 1973 to 1977.  For example, we show that we are able to detect and characterize the Fall of Saigon.
\end{abstract}


\section{Introduction}

Communications between the U.S. State Department and its embassies have historically been called \emph{diplomatic cables}, derived from the time when physical cables were used for such communications.
We obtained around two million of these cables sent between 1973 and 1977 via the History Lab at Columbia,\footnote{http://history-lab.org} which received them from the Central Foreign Policy Files at the National Archives.  In addition to the text of the cables themselves, each document is supplemented with information about who sent the cable (e.g., the State Department, the U.S. Embassy in Saigon, or an individual by name), who received the message (often multiple entities), and the date the message was sent.
Figure~\ref{fig:cables_example} shows three cables which were sent in April 1975 concerning orphans from the Vietnam War.


\begin{figure}[ht]
\includegraphics[width=\textwidth]{fig/cables_orphan_example.png}
\caption{Example excerpts of cables.}
\label{fig:cables_example}
\end{figure}

Historians and political scientists are interested in the process of sending cables---of particular interest is identifying historical events in a collection of these cables.  These experts want to know \emph{when} events happen, as well as \emph{what} happens during the event, which includes the parties sending and receiving messages (\emph{who}). 

For this work, we characterize an event in two ways: \emph{when} it occurs and \emph{what} occurs.  We assume that only one event can begin on any given day, which allows us to describe each day in terms of the probability of an event starting that day.  We can also assume that an event will influence cables for a set time period after it starts---this way, we only need to discover the starting day for each event.  
To understand \emph{what} occurs, we can summarize the cable message content with a topic model such as LDA~\citep{Blei:2003} and model event content in that same space.


\section{A Generative Model of Events}
Our model is a generative process---we make assumptions about how the data came to be and describe these assumptions in terms of probability distributions.  Given our model and observed data, the task is then to reverse the generative process to find the hidden quantities that (retrospectively) generated the data.

Consider an entity like the Bangkok American embassy.  We can imagine that there is a stream of cables being sent by this embassy---some might be sent to the US State Department, others to another American embassy like Hong Kong, and perhaps a few are sent to individuals by name.  Each of these cables has an associated send date, and we can represent the content of the cable with a topic model; we call these cable descriptions $\theta$, which is a matrix of $D$ cables (or documents) by $K$ topics.\footnote{This allows us to represent the cable in terms of about 100 topics rather than in terms of hundreds of thousands of vocabulary words. We can discover these topics with LDA and treat them as fixed observations going forward.}

An entity will usually talk about certain topics and with certain frequency. The Bangkok embassy, for instance, sent and average of 22 cables per day in the 1970s, and was concerned with topics regarding southeast Asia more generally.  We can describe the general interests of entities in the same topic space we use to describe individual cables and we will call these per-entity interests $\phi$ and can assign them a formal distribution.
\begin{itemize}
\item Draw the entity's base topics: $\phi_{0k} \sim \mbox{Gamma}(\alpha, \beta)$
\end{itemize}
Eventually we will want to model the interactions between entities, but for now we can consider a single entity at a time.

Now imagine that at a particular time, an event occurs, such as the capture of Saigon during the Vietnam war.  We do not directly observe that events occurs, but each event can again be described in the same topic space used to describe individual cables.  Whether or not an event occurs at a particular time step is represented by $\epsilon_t$ and the content of the event (or its topical representation) is called $\pi_t$.

\begin{itemize}
\item For each day $i$ with date $a_i$:
\begin{itemize}
\item Generate event occurrence/strength $\epsilon \sim \mbox{Poisson}
(\eta_\epsilon)$, where $\eta_\epsilon$ is a fixed, non-negative
hyperparameter for the mean event strength.
\item Generate the day/event's description in terms of each topic $k$: $\pi_{ik} \sim \mbox{Gamma}(\alpha_0, \beta_0)$, where $\alpha_0$ and $\beta_0$ are fixed hyperparameters.
\end{itemize}
\end{itemize}

When an event occurs, both the frequency of cables being sent and the cable content changes. The Bangkok embassy sent 31 cables the day following the capture of Saigon (a 36\% increase over the average), and the majority of these cables are about Vietnam war refugees.  Thus we imagine that an entity's stream of cables is controlled by what it usually talks about (and how often) as well as the higher level stream of unobserved events.  The influence of an event does not last indefinitely, however, so we model the decay of its magnitude with some function $f$.
While many decay functions can be used, we define the event decay function to be a simple linear decrease: \[f(a, c) =
\begin{cases}
	1 - \frac{c-a}{d}, & \mbox{if } a \le c < a+d \\
	0, & \mbox{otherwise,}
\end{cases} \]
where $d$ is the time distance (in days) after event $a$ at which point the event is no longer relevant.

When we analyze the cables with this model setup, we disentangle cables that represent ``business as usual'' from those that reflect the higher-order event stream.  Consequently, we infer what that stream is,
i.e., when something happened and what happened.

Recall that the key hidden values are event descriptions $\pi$ (``what''), event occurrences $\epsilon$ (``when''), and entity interests $\phi$.  (Since entities are tied to individuals and places, we can use them to describe ``who'' is involved and ``where'' and event occurs after fitting our model.)  These are all tied together when we model the observed data itself, as we assume that each cable is generated from a combination of these parameters.
\begin{itemize}
\item For each cable $j$ on date $c_j$:
\begin{itemize}
	\item Set cable topic parameter: $\phi_{jk} = \phi_{0k} + \sum_i f(a_i, c_j) \pi_{ik} \epsilon_i$.
	\item Draw cable topic: $\theta_{jk} \sim \mbox{Gamma}(\beta_c \phi_{jk}, \beta_c)$.
\end{itemize}
\end{itemize}



\subsection{Inference}
Posterior inference is the central computational problem.  We want to learn the hidden values describe above (event descriptions $\pi$, event occurrences $\epsilon$, and entity descriptions $\phi$) from our observed data.  We construct a black box variational inference algorithm following~\citet{Ranganath:2014} to determine the values of these latent parameters.  Full details on the derivation of this inference algorithm can be found in Appendix~\ref{sec:appendix_inference}.

\section{Validation}
TODO

\section{Discussion}
Once we have determined the values of the hidden parameters in our model, we will have a discrete list of events in terms of when they occur and their topical content.  We plan to compare these discovered events to a list of predetermined historical events to evaluate the effectiveness of our model.

Traditional topic models can describe documents, but they cannot identify when events occur---only a model like ours that explicitly models event occurrences and event content can attribute document observations to discrete events.  Further, by modeling entities, we can distinguish between ``business-as-usual'' document content and content that is attached to particular events---we are also unable to capture this phenomenon with traditional topic models.

%needs vlaidation: how to check our model is doing what we want; compare to real, known, events



\bibliographystyle{apa}
\bibliography{/Users/ajbc/Dropbox/papers/library.bib}


\newpage
\appendix
\section{Inference Details}
\label{sec:appendix_inference}
For now, we assume that we know the LDA topics $\beta$ and only observe the documents in terms of their topics $\theta$; breaking this assumption makes inference a little more complicated as the updates for $\theta$ would have new dependencies.

As usual, inference is the central computational problem.  Variational inference approaches this problem by minimizing the KL divergence from an approximating distribution $q$ to the true posterior $p$.
This is equivalent to maximizing the ELBO: \[\mathcal{L}(q)  = \E_{q(\epsilon, \pi, \phi)}[\log p(\theta,\epsilon,\pi,\phi) - \log q(\epsilon, \pi, \phi)].\]

We define the approximating distribution $q$ using the mean field assumption: \[q(\epsilon, \pi, \phi) = \prod_i q(\epsilon_{i})\prod_k\left[q(\phi_{0k})\prod_i q(\pi_{ik})\right].\]
The variational distributions $q(\pi)$ and $q(\phi)$ are both gamma-distributed with free variational parameters $\lambda^\pi$ and $\lambda^\phi$, respectively.  Because these parameters are free, we use the softmax function $\mathcal{M}(x) = \log(1+\exp(x))$ to constrain them so that they do not violate the requirements of the gamma distribution.
The variational distribution $q(\epsilon)$ is Poisson-distributed with variational parameter $\lambda^\epsilon$; this free parameter is also constrained by the softmax function.

The expectations under $q$, which are needed to maximize the ELBO, do not have a simple analytic form, so we use ``black box'' VI techniques.
F or each variable, we can write the log probability of all terms containing that variable, giving us 
\[\log p^\epsilon_{i}(\theta, \epsilon, \pi, \phi) \triangleq \log p(\epsilon_{i} \g \eta_\epsilon) + \sum_{j : f(a_i, c_j) \neq 0}\sum_k\log p(\theta_{jk} \g \phi_{0k}, c_j, a_i, d, \beta_c, \pi_{ik}, \epsilon_i),\]
\[\log p^\pi_{ik}(\theta, \epsilon, \pi, \phi) \triangleq \log p(\pi_{ik} \g \alpha_0, \beta_0) + \mathbf{1}[\epsilon_i \neq 0] \sum_{j : f(a_i, c_j) \neq 0}\log p(\theta_{jk} \g \phi_{0k}, c_j, a_i, d, \beta_c, \pi_{ik}, \epsilon_i),\]
and
\[p^\phi_{k}(\theta, \epsilon, \pi, \phi) \triangleq \log p(\phi_{0k} \g \alpha, \beta) + \sum_{j} \log p(\theta_{jk} \g \phi_{0k}, c_j, a_i d, \beta_c, \pi_{ik}, \epsilon_i).\]
Then we can write the gradients with respect to the variational parameters as:
\[\nabla_{\lambda^\epsilon_{i}} \mathcal{L} = \E_q \left[ \nabla_{\lambda^\epsilon_{i}} \log q(\epsilon_{i} \g \lambda^\epsilon_{i}) \left( \log p^\epsilon_{i}(\theta, \epsilon, \pi, \phi) - \log q(\epsilon_{i} \g \lambda^\epsilon_{i}) \right)\right],\]
\[\nabla_{\lambda^\pi_{ik}} \mathcal{L} = \E_q \left[ \nabla_{\lambda^\pi_{ik}} \log q(\pi_{ik} \g \lambda^\pi_{ik}) \left( \log p^\pi_{ik}(\theta, \epsilon, \pi, \phi) - \log q(\pi_{ik} \g \lambda^\pi_{ik}) \right)\right],\]
and
\[\nabla_{\lambda^\phi_{k}} \mathcal{L} = \E_q \left[ \nabla_{\lambda^\phi_{k}} \log q(\phi_{0k} \g \lambda^\phi_{k}) \left( \log p^\phi_{k}(\theta, \epsilon, \pi, \phi) - \log q(\phi_{0k} \g \lambda^\phi_{k}) \right)\right].\]

Using this framework, we construct our black box algorithm below. 

\paragraph{For Reference}
The gamma distribution and derivatives:
\begin{align}
\log\mbox{Gamma}(x \g a,b) & = ab\log b - \log\Gamma \left(ab\right) +(ab-1)\log x - bx \label{eq:gamma}\\
%\log\mbox{Gamma}(x \g \mathcal{M}(a), \mathcal{M}(b)) & = 
%\mathcal{M}(a)\mathcal{M}(b)\log \mathcal{M}(b) - \log\Gamma \left(\mathcal{M}(a)\mathcal{M}(b)\right) +(\mathcal{M}(a)\mathcal{M}(b)-1)\log x - \mathcal{M}(b)x \\
\nabla_a \log\mbox{Gamma}(x \g \mathcal{M}(a), \mathcal{M}(b)) &= 
\mathcal{M}'(a) \mathcal{M}(b) \left[ \log \mathcal{M}(b) - \Psi \left(\mathcal{M}(a)\mathcal{M}(b)\right) + \log x\right] \label{eq:dgammaA}
\\
\nabla_b \log\mbox{Gamma}(x \g \mathcal{M}(a), \mathcal{M}(b)) &= 
\mathcal{M}'(b) \left[ \mathcal{M}(a) \left((\log \mathcal{M}(b) + 1) - \Psi \left(\mathcal{M}(a)\mathcal{M}(b)\right) + \log x\right) - x\right]\label{eq:dgammaB}
%\frac{d}{d b}\log\mbox{Gamma}(x \g \mathcal{M}(a), \mathcal{M}(b)) &= 
%\mathcal{M}'(b) \left[ \mathcal{M}(a) (\log \mathcal{M}(b) + 1) - \mathcal{M}(a)\Psi \left(\mathcal{M}(a)\mathcal{M}(b)\right) + \mathcal{M}(a)\log x - x\right]
\end{align}

The Poisson distribution and derivative:
\begin{align}
\log\mbox{Poisson}(x~\vert~\lambda) &= x\log\lambda - \log(x!) - \lambda \label{eq:poisson} \\
%\log\mbox{Poisson}(x~\vert~\mathcal{M}(\lambda)) &= x\log\mathcal{M}(\lambda) - \log(x!) -\mathcal{M}(\lambda) \\
\nabla_\lambda \log\mbox{Poisson}(x~\vert~\mathcal{M}(\lambda)) &= \mathcal{M}'(\lambda)\left[\frac{x}{\mathcal{M}(\lambda)} -1\right].\label{eq:dpoisson}
\end{align}

The softmax function and derivative:
\[\mathcal{M}(x) = \log(1+e^x)\]
\[\mathcal{M}'(x) = \frac{e^x}{1+e^x}\]
%\SetCommentSty{mycommfont}

\begin{algorithm}
\small
\DontPrintSemicolon
\KwIn{document topics $\theta$}
\KwOut{estimates of latent parameters entity topics $\phi$, event topics $\pi$,
and event occurances $\epsilon$}
\textbf{Initialize} $\lambda^\pi$, $\lambda^\phi$, and $\lambda^\epsilon$ to
respective priors \;
\textbf{Initialize} iteration count $t = 0$ \;
\While {change in validation likelihood $< \delta$}{
	initialize $\sigma^\pi = 0$ \;
	\For {each sample $s = 1, \dots, S$}{
		\For {each component $k$} {
			draw sample entity topics $\phi_{0k}[s] \sim \mbox{Gamma}(\mathcal{M}(\lambda^\phi_{k}))$ \;
			\BlankLine
			set $p^\phi_k[s] = \log p(\phi_{0k}[s] \g \alpha, \beta)$ \tcp*{see Eqn.~\ref{eq:gamma}}
			set $q^\phi_k[s] = \log q(\phi_{0k}[s] \g \lambda^\phi_k)$ \tcp*{Eqn.~\ref{eq:gamma} with params $\mathcal{M}(\lambda^{\phi}_k)$}
			set $g^\phi_k[s] = \nabla_{\lambda^\phi_{k}} \log q(\phi_{0k}[s] \g \lambda^\phi_{k})$ \tcp*{see Eqns.~\ref{eq:dgammaA}, \ref{eq:dgammaB}}
		}
		\For {each timestep $i$} {
			draw sample event occurance $\epsilon_i[s] \sim \mbox{Poisson}(\mathcal{M}(\lambda^\epsilon_i))$ \;
			\BlankLine
			set $p^\epsilon_i[s] = \log p(\epsilon_{i}[s] \g \eta)$ \tcp*{see Eqn.~\ref{eq:poisson}}
			set $q^\epsilon_i[s] = \log q(\epsilon_i[s] \g \lambda^\epsilon_i)$ \tcp*{Eqn.~\ref{eq:poisson} with param $\mathcal{M}(\lambda^{\epsilon}_i)$}
			set $g^\pi_{i}[s] = \nabla_{\lambda^\epsilon_{i}} \log q(\epsilon_{i}[s] \g \lambda^\epsilon_{i})$ \tcp*{see Eqn.~\ref{eq:dpoisson}}
			\BlankLine
			\If {$\epsilon_i[s] \neq 0$} {
				\For {each component $k$} {
					draw sample event topics $\pi_{ik}[s] \sim \mbox{Gamma}(\mathcal{M}(\lambda^\pi_{ik}))$ \;
					\BlankLine
					set $p^\pi_{ik}[s] = \log p(\pi_{ik}[s] \g \alpha_0, \beta_0)$ \tcp*{see Eqn.~\ref{eq:gamma}}
					set $q^\pi_{ik}[s] = \log q(\pi_{ik}[s] \g \lambda^\pi_{ik})$ \tcp*{Eqn.~\ref{eq:gamma} with params $\mathcal{M}(\lambda^{\pi}_{ik})$}
					set $g^\pi_{ik}[s] = \nabla_{\lambda^\pi_{ik}} \log q(\pi_{ik}[s] \g \lambda^\pi_{ik})$ \tcp*{see Eqns.~\ref{eq:dgammaA}, \ref{eq:dgammaB}}
				}
			}
		}
	}
	
	\For {each document $j$ (sent on date $c_j$ and has topics $\theta_j$), sample $s$ and component $k$}{
		set $\phi_{jk}[s] = \phi_j[s] + \sum_i f(a_i, c_j) \epsilon_i[s] \pi_{ik}[s]$ \;
		set $p^\theta_{jk}[s] = \log p(\theta_{jk} \g \phi_{jk}[s], \beta_c)$ \tcp*{see Eqn.~\ref{eq:gamma}}
		\BlankLine
		$p^\phi_k[s] \pluseq p^\theta_{jk}[s]$ \;
		\For {each timestep $i$ where $a_i \le c_j < a_i + d$}{
			$p^\epsilon_{i}[s] \pluseq \sum_k p^\theta_{jk}[s]$ \;
			\If {$\epsilon_i[s] \neq 0$} {
				$p^\pi_{ik}[s] \pluseq p^\theta_{jk}[s]$ \;
				update $\sigma^\pi_i \pluseq 1$ \;
			}
		}
	}

	
	set $\hat\nabla_{\lambda^{\phi} }\mathcal{L} \triangleq \frac{1}{S} \sum_s g^\phi[s] ( p^\phi[s] -  q^\phi[s] )$ \;
	set $\hat\nabla_{\lambda^{\epsilon}} \mathcal{L} \triangleq \frac{1}{S} \sum_s g^\epsilon[s] ( p^\epsilon[s] -  q^\epsilon[s] )$ \;
	set $\hat\nabla_{\lambda^{\pi}} \mathcal{L} \triangleq \frac{1}{\sigma_\pi} \sum_s g^\pi[s] ( p^\pi[s] -  q^\pi[s] )$ \;

	\BlankLine
	set $\rho= (t +\tau)^\kappa$ \;
	% update event content for each event $i$ and topic $k$: 
	set $\lambda^{\pi} \pluseq \rho \hat\nabla_{\lambda^{\pi}} \mathcal{L}$ \;
	set $\lambda^{\epsilon} \pluseq \rho \hat\nabla_{\lambda^{\epsilon}} \mathcal{L}$ \;
	set $\lambda^{\phi} \pluseq \rho \hat\nabla_{\lambda^{\phi}} \mathcal{L}$ \;
}

set $\E[\pi] = \lambda^{\pi,a}$ \;
set $\E[\phi] = \lambda^{\phi,a}$ \;
set $\E[\epsilon] = \lambda^{\epsilon}$ \;
\BlankLine
\Return{$\E[\pi]$, $\E[\phi]$, $\E[\epsilon]$} \;
\caption{Inference for Cables Model}
\label{alg:cables}
\end{algorithm}


\end{document}
