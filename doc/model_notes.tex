\input{preamble/preamble.tex}
\input{preamble/preamble_math.tex}
\input{preamble/preamble_acronyms.tex}

\begin{document}

\title{Modeling Cables Events}

\author{Allison J.B. Chaney}

\maketitle

\section{Generative model} We start with a fitted LDA model where documents
are represented in terms of topics ($\theta$, a $D \times K$ matrix), and
topics are represented as a distribution over words ($\beta$, a $K \times V$
matrix).  This model fit, along with document metadata, are our observations.
Alternatively, we can tack on the LDA generative process to the model below.

\begin{itemize}
\item for each day $i$ with date $a_i$:
\begin{itemize}
\item generate event occurrence/strength $\epsilon \sim \mbox{Poisson}
(\eta_\epsilon)$, where $\eta_\epsilon$ is a fixed, non-negative
hyperparameter for the mean event strength
\item generate the day/event's description in terms of each topic $k$: $\pi_{ik} \sim \mbox{Gamma}(\alpha_0, \beta_0)$, where $\alpha_0$ and $\beta_0$ are fixed hyperparameters.
\end{itemize}
\item draw the entity's base topics: $\phi_{0k} \sim \mbox{Gamma}(\alpha, \beta)$ (eventually for each entity, but for now, just limit data to only one entity)
\item For each cable $j$ on date $c_j$:
\begin{itemize}
	\item set cable topic parameter: $\phi_{jk} = \phi_{0k} + \sum_i f(a_i, c_j) \pi_{ik} \epsilon_i$, where $f$ is defined below.
	\item draw cable topic: $\theta_{jk} \sim \mbox{Gamma}(\beta_c \phi_{jk}, \beta_c)$
\end{itemize}
\end{itemize}

\noindent Note that\[f(a, c) =
\begin{cases}
	1 - \frac{c-a}{d}, & \mbox{if } a \le c < a+d \\
	0, & \mbox{otherwise,}
\end{cases} \]
where $d$ is the time distance (in days) after event $a$ at which point the event is no longer relevant.


\section{Inference}
For now, we assume that we know the LDA topics $\beta$ and only observe the documents in terms of their topics $\theta$; breaking this assumption makes inference a little more complicated as the updates for $\theta$ would have new dependencies.  Pending the results, we should explore that vein. 

Here, we follow the structure of the DEF paper to explain inference for this model.

\begin{itemize}
\item As usual, inference is the central computational problem.
\item Variational inference minimizes the KL divergence from an approximating distribution $q$ to the true posterior $p$.
\item This is equivalent to maximizing the ELBO: $\mathcal{L}(q)  = \E_{q(\epsilon, \pi, \phi)}[\log p(\theta,\epsilon,\pi,\phi) - \log q(\epsilon, \pi, \phi)]$
\item we define the approximating distribution $q$ using the mean field assumption: $q(\epsilon, \pi, \phi) = \prod_i q(\epsilon_{i})\prod_k\left[q(\phi_{0k})\prod_i q(\pi_{ik})\right]$ 
\item $q(\pi)$ and $q(\phi)$ are both gamma-distributed with variational parameters $\lambda^\pi$ and $\lambda^\phi$, respectively, where we use the softmax function $\mathcal{M}(x) = \log(1+\exp(x))$ to constrain the free variational parameters.
$q(\epsilon)$ is Poisson-distributed with variational parameter $\lambda^\epsilon$; the free parameter is also constrained by the softmax function.

\item the expectations under $q$ (needed to maximize the ELBO) do not have a simple analytic form, so we use ``black box'' VI techniques
\item for each variable, we can write the log probability of all terms containing that variable, giving us 
\[\log p^\epsilon_{i}(\theta, \epsilon, \pi, \phi) = \log p(\epsilon_{i} \g \eta_\epsilon) + \sum_{j : f(a_i, c_j) \neq 0}\sum_k\log p(\theta_{jk} \g \phi_{0k}, c_j, a_i, d, \beta_c, \pi_{ik}, \epsilon_i),\]
\[\log p^\pi_{ik}(\theta, \epsilon, \pi, \phi) = \log p(\pi_{ik} \g \alpha_0, \beta_0) + \sum_{j : f(a_i, c_j) \neq 0}\log p(\theta_{jk} \g \phi_{0k}, c_j, a_i, d, \beta_c, \pi_{ik}, \epsilon_i),\]
and
\[p^\phi_{k}(\theta, \epsilon, \pi, \phi) = \log p(\phi_{0k} \g \alpha, \beta) + \sum_i\sum_{j} \log p(\theta_{jk} \g \phi_{0k}, c_j, a_i d, \beta_c, \pi_{ik}, \epsilon_i).\]
\item Then we can write the gradients with respect to the variational parameters as:
\[\nabla_{\lambda^\epsilon_{i}} \mathcal{L} = \E_q \left[ \nabla_{\lambda^\epsilon_{i}} \log q(\epsilon_{i} \g \lambda^\epsilon_{i}) \left( \log p^\epsilon_{i}(\theta, \epsilon, \pi, \phi) - \log q(\epsilon_{i} \g \lambda^\epsilon_{i}) \right)\right],\]
\[\nabla_{\lambda^\pi_{ik}} \mathcal{L} = \E_q \left[ \nabla_{\lambda^\pi_{ik}} \log q(\pi_{ik} \g \lambda^\pi_{ik}) \left( \log p^\pi_{ik}(\theta, \epsilon, \pi, \phi) - \log q(\pi_{ik} \g \lambda^\pi_{ik}) \right)\right],\]
and
\[\nabla_{\lambda^\phi_{k}} \mathcal{L} = \E_q \left[ \nabla_{\lambda^\phi_{k}} \log q(\phi_{0k} \g \lambda^\phi_{k}) \left( \log p^\phi_{k}(\theta, \epsilon, \pi, \phi) - \log q(\phi_{0k} \g \lambda^\phi_{k}) \right)\right].\]
\end{itemize}

Using this framework, we construct our black box algorithm below. 

\paragraph{For Reference}
The gamma distribution and derivatives:
\begin{align}
\log\mbox{Gamma}(x \g a,b) & = ab\log b - \log\Gamma \left(ab\right) +(ab-1)\log x - bx \label{eq:gamma}\\
%\log\mbox{Gamma}(x \g \mathcal{M}(a), \mathcal{M}(b)) & = 
%\mathcal{M}(a)\mathcal{M}(b)\log \mathcal{M}(b) - \log\Gamma \left(\mathcal{M}(a)\mathcal{M}(b)\right) +(\mathcal{M}(a)\mathcal{M}(b)-1)\log x - \mathcal{M}(b)x \\
\nabla_a \log\mbox{Gamma}(x \g \mathcal{M}(a), \mathcal{M}(b)) &= 
\mathcal{M}'(a) \mathcal{M}(b) \left[ \log \mathcal{M}(b) - \Psi \left(\mathcal{M}(a)\mathcal{M}(b)\right) + \log x\right] \label{eq:dgammaA}
\\
\nabla_b \log\mbox{Gamma}(x \g \mathcal{M}(a), \mathcal{M}(b)) &= 
\mathcal{M}'(b) \left[ \mathcal{M}(a) \left((\log \mathcal{M}(b) + 1) - \Psi \left(\mathcal{M}(a)\mathcal{M}(b)\right) + \log x\right) - x\right]\label{eq:dgammaB}
%\frac{d}{d b}\log\mbox{Gamma}(x \g \mathcal{M}(a), \mathcal{M}(b)) &= 
%\mathcal{M}'(b) \left[ \mathcal{M}(a) (\log \mathcal{M}(b) + 1) - \mathcal{M}(a)\Psi \left(\mathcal{M}(a)\mathcal{M}(b)\right) + \mathcal{M}(a)\log x - x\right]
\end{align}

The Poisson distribution and derivative:
\begin{align}
\log\mbox{Poisson}(x~\vert~\lambda) &= x\log\lambda - \log(x!) - \lambda \label{eq:poisson} \\
%\log\mbox{Poisson}(x~\vert~\mathcal{M}(\lambda)) &= x\log\mathcal{M}(\lambda) - \log(x!) -\mathcal{M}(\lambda) \\
\nabla_\lambda \log\mbox{Poisson}(x~\vert~\mathcal{M}(\lambda)) &= \mathcal{M}'(\lambda)\left[\frac{x}{\mathcal{M}(\lambda)} -1\right].\label{eq:dpoisson}
\end{align}

The softmax function and derivative:
\[\mathcal{M}(x) = \log(1+e^x)\]
\[\mathcal{M}'(x) = \frac{e^x}{1+e^x}\]


\begin{algorithm}
\DontPrintSemicolon
\KwIn{document topics $\theta$}
\KwOut{estimates of latent parameters entity topics $\pi$, event topics $\phi$,
and event occurances $\epsilon$}
\textbf{Initialize} $\lambda^\pi$, $\lambda^\phi$, and $\lambda^\epsilon$ to
respective priors \;
\textbf{Initialize} iteration count $t = 0$ \;
\While {change in validation likelihood $< \delta$}{
	$t \pluseq 1$ \;
	initialize $\hat\lambda^{\pi}$, $\hat\lambda^{\phi}$, and 
		$\hat\lambda^{\epsilon}$ to 0 matrices \;
	\For {$j=1, \dots, B$ (batch size, or \# of document samples)} {
		Sample a document $j$ that is sent on date $c_j$ and has topics 
			$\theta_j$ \;
			\For {$s=1, \dots, S$} {
				test \;

			}
%         			\For {$k=1, \dots,K$}
%         				\State draw sample $\phi_{0k}[s] \sim \mbox{Gamma}(\mathcal{M}(\lambda^\phi_{k}))$
%         				\State set $p^\phi_k[s] = \log p(\phi_{0k}[s] \g \alpha, \beta)$  \Comment{see Eqn.~\ref{eq:gamma}}
%  	       			\State set $q^\phi_k[s] = \log q(\phi_{0k} \g \lambda^\phi_k)$ \Comment{Eqn.~\ref{eq:gamma} with parameters $\mathcal{M}(\lambda^{\phi,a}_k),\mathcal{M}(\lambda^{\phi,b}_k)$}
%         				\State set $g^\phi_k[s] = \nabla_{\lambda^\phi_{k}} \log q(\phi_{0k}[s] \g \lambda^\phi_{k})$  \Comment{see Eqns.~\ref{eq:dgammaA}, \ref{eq:dgammaB}}
%         				\For {each event $i$ on date $a_i \in \left( c_j - d, c_j \right]$}
%         					\State draw sample $\pi_{ik}[s] \sim \mbox{Gamma}(\mathcal{M}(\lambda^\pi_{ik}))$
%         					\State draw sample $\epsilon_i[s] \sim \mbox{Poisson}(\mathcal{M}(\lambda^\epsilon_i))$
%  	       				\State set $p^\pi_{ik}[s] = \log p(\pi_{ik}[s] \g \alpha_0, \beta_0)$  \Comment{see Eqn.~\ref{eq:gamma}}
%         					\State set $q^\pi_{ik}[s] = \log q(\pi_{ik}[s] \g \lambda^\pi_{ik})$\Comment{Eqn.~\ref{eq:gamma} with parameters $\mathcal{M}(\lambda^{\pi,a}_{ik}),\mathcal{M}(\lambda^{\pi,b}_{ik})$}
%         					\State set $g^\pi_{ik}[s] = \nabla_{\lambda^\pi_{ik}} \log q(\pi_{ik}[s] \g \lambda^\pi_{ik})$\Comment{see Eqns.~\ref{eq:dgammaA}, \ref{eq:dgammaB}}
%         				\EndFor
%         			\EndFor
%         			\For {each event $i$ on date $a_i \in \left( c_j - d, c_j \right]$}
%         				\State set $p^\epsilon_i[s] = \log p(\epsilon_{i}[s] \g \eta)$ \Comment{see Eqn.~\ref{eq:poisson}}
%         				\State set $q^\epsilon_i[s] = \log q(\epsilon_i \g \lambda^\epsilon_i)$ \Comment{Eqn.~\ref{eq:poisson} with parameter $\mathcal{M}(\lambda^{\epsilon}_i)$}
%         				\State set $g^\pi_{i}[s] = \nabla_{\lambda^\epsilon_{i}} \log q(\epsilon_{i}[s] \g \lambda^\epsilon_{i})$\Comment{see Eqn.~\ref{eq:dpoisson}}
%         			\EndFor
		
% 			\For {$k=1, \dots,K$}
% 				\State set $\phi_{jk}[s] = \phi_{0k}[s] + \sum_{i : a_i \in \left( c_j - d, c_j \right]}\epsilon_i[s] \pi_{ik}[s] f(a_i, c_j)$
% 				\State set $p^\theta_k[s] = \log p(\theta_j \g \phi_{jk}[s], \beta_c)$\Comment{see Eqn.~\ref{eq:gamma}}
% 				\For {each event $i$}
% 					\State update $\hat\lambda^{\pi}_{ik} \pluseq \frac{1}{SB} g^\pi_{ik}[s] ( p^\pi_{ik}[s] + p^\theta_k[s] -  q^\pi_{ik}[s] ) $
% 				\EndFor
% 				\State update $\hat\lambda^{\phi}_{k} \pluseq \frac{1}{SB} g^\phi_{ik}[s] ( p^\phi_{k}[s] + p^\theta_k[s] -  q^\phi_{k}[s] ) $
% 			\EndFor
% 			\For {each event $i$}
% 				\State update $\hat\lambda^{\epsilon}_{i} \pluseq \frac{1}{SB} g^\epsilon_{i}[s] ( p^\epsilon_{i}[s] + \sum_k p^\theta_k[s] -  q^\epsilon_{i}[s] ) $
% 			\EndFor
			
% 		\EndFor
	}
	set $\rho= (t +\tau)^\kappa$ \;
	update event content for each event $i$ and topic $k$: $\lambda^{\pi}_{ik} 
		= \lambda^{\pi}_{ik} + \rho \hat\lambda^{\pi}_{ik}$ \;
	update general entity topics for each topic $k$: $\lambda^{\phi}_{k} = 
		\lambda^{\phi}_{k} + \rho \hat\lambda^{\phi}_{k}$ \;
	update event occurrences for each event $i$: $\lambda^{\epsilon}_{k} = 
		\lambda^{\epsilon}_{i} + \rho \hat\lambda^{\epsilon}_{i}$ \;
	\BlankLine
	set $\E[\pi] = \lambda^{\pi,a}$ \;
	set $\E[\phi] = \lambda^{\phi,a}$ \;
	set $\E[\epsilon] = \lambda^{\epsilon}$ \;
}
\Return{$\E[\pi]$, $\E[\phi]$, $\E[\epsilon]$} \;
\caption{Inference for Cables Model}
\label{alg:cables}
\end{algorithm}



\end{document}
