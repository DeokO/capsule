%!TEX root = supplemental.tex

In this appendix, we describe the details of the approximate inference
algorithm for Capsule.

Conditioned on the observed term counts---$n_{dv}$ for vocabulary term
$v$ in message $d$; collectively $\mathbf{N}$---our goal is to learn
the posterior distribution of the latent variables. Each message is
associated with an author entity $a_d$ and a time interval $t_d$
within which that messages was sent. The latent variables are the
general topics $\mathbold{\beta}_1, \ldots, \mathbold{\beta}_K$, the
entity topics $\mathbold{\eta}_1, \ldots, \mathbold{\eta}_A$, and the
event topics $\mathbold{\gamma}_1, \ldots, \mathbold{\gamma}_T$, as
well as the message-specific strengths
$\mathbold{\theta}_{1}, \ldots, \mathbold{\theta}_D$,
$\zeta_{1}, \ldots, \zeta_D$, and
$\mathbold{\epsilon}_{1}, \ldots, \mathbold{\epsilon}_D$, the
entity-specific strengths
$\mathbold{\phi}_1, \ldots, \mathbold{\phi}_A$ and
$\xi_1, \ldots, \xi_A$, and the event strengths
$\psi_1, \ldots, \psi_T$. See \cref{fig:graphicalmodel,fig:generative-model}
for the graphical model and generative process.

As for many Bayesian models, the posterior distribution is not
tractable to compute; we must instead approximate it. We therefore
introduce an approximate inference algorithm for Capsule, based on
variational methods~\cite{Jordan:1999,Wainwright:2008}. Variational
methods approximate the true posterior distribution $p$ with a
(simpler) variational distribution $q$. Inference then consists of
minimizing the KL divergence from $q$ to $p$. This is equivalent to
maximizing the evidence lower bound (ELBO):
\begin{equation}
\mathcal{L}(q)  = \E_{q}\left[\log{
p(\mathbf{N}, \mathbold{\beta}, \mathbold{\eta}, \mathbold{\gamma}, \mathbold{\theta}, \mathbold{\zeta}, \mathbold{\epsilon}, \mathbold{\phi}, \mathbold{\xi},
\mathbold{\psi})}
- \log{q(\mathbold{\beta}, \mathbold{\eta}, \mathbold{\gamma}, \mathbold{\theta}, \mathbold{\zeta}, \mathbold{\epsilon}, \mathbold{\phi}, \mathbold{\xi},
\mathbold{\psi})}\right].  \label{eq:cables:elbo}
\end{equation}

We define $q$ using the mean field assumption:
\begin{align}
&q(\mathbold{\beta}, \mathbold{\eta}, \mathbold{\gamma}, \mathbold{\theta}, \mathbold{\zeta}, \mathbold{\epsilon}, \mathbold{\phi}, \mathbold{\xi},
\mathbold{\psi})
= \notag\\
&\quad
        	 	\prod_{d=1}^D \left(q(\zeta_d \g \lambda_d)
				\prod_{k=1}^K q(\theta_{dk} \g \lambda^\theta_{dk})
				\prod_{t=1}^T q(\epsilon_{dt} \g \lambda^\epsilon_{dt})
			\right) \times{}\notag \\
&\quad \prod_{k=1}^K \left(
q(\mathbold{\beta}_k \g \lambda_k^{\beta}) \prod_{a=1}^A
q(\phi_{ak} \g \lambda_{ak}^{\phi})\right) \prod_{a=1}^A \left(q(\mathbold{\eta}_a
\g \lambda_a^{\eta})\,q(\xi_a \g \lambda_a^{\xi}) \right) \prod_{t=1}^T \left(
q(\mathbold{\gamma}_t \g \lambda_t^{\gamma})\,q(\psi_t \g \lambda_t^{\gamma}\right)
	\label{eq:q}
\end{align}

The variational distributions for the topics $q(\mathbold{\beta}_k)$,
$q(\mathbold{\eta}_a)$, and $q(\mathbold{\gamma}_t)$ are all Dirichlet
distributions with free variational parameters $\lambda_{k}^{\beta}$,
$\lambda_a^{\eta}$, and $\lambda_t^{\gamma}$, respectively. The
variational distributions for the strengths $q(\theta_{dk})$,
$q(\zeta_{d})$, $q(\epsilon_{dt})$, $q(\phi_{ak})$, $q(\xi_a)$, and
$q(\psi_t)$ are all gamma distributions with free variational
parameters $\lambda_{dk}^{\theta}$, $\lambda_d^{\zeta}$,
$\lambda_{dt}^{\epsilon}$, $\lambda_{ak}^{\phi}$, $\lambda_a^{\xi}$,
and $\lambda_{t}^{\psi}$, respectively. Each of these parameters has
two components: shape $s$ and rate $r$.

The expectations under $q$, which we need to maximize the ELBO, have
closed analytic forms. We therefore update each free variational
parameter in turn, following a standard coordinate-ascent approach.

To obtain update equations for the free variational parameters, we
introduce auxiliary latent variables:
\begin{align}
z_{dkv}^{\mathcal{K}}
&\sim \textrm{Poisson}\left(\theta_{dk}\beta_{kv}\right)\\
z_{dv}^{\mathcal{A}}
&\sim \textrm{Poisson}\left(\zeta_{d}\eta_{a_d v}\right)\\
z_{dtv}^{\mathcal{T}}
&\sim \textrm{Poisson}\left(f(t_d, t)\,\epsilon_{d t} \gamma_{tv}\right),
\end{align}
where the superscripts $\mathcal{K}$, $\mathcal{A}$, and $\mathcal{T}$
indicate the general, entity, and event topics, respectively. When
marginalized out, these variables leave the original model
specification intact. Because the Poisson distribution has an additive
property, the value of $n_{dv}$ is completely determined by the values
of these variables:
\begin{equation}
n_{dv} = \sum_{k=1}^K z_{dkv}^{\mathcal{K}} + z_{dv}^{\mathcal{A}}
+ \sum_{t=1}^T z_{dtv}^{\mathcal{T}}.
\end{equation}

Coordinate-ascent variational inference depends on the conditional
distribution of each latent variable given the values of the other
latent variables and the data. We use $D(a)$ to denote the set of
messages sent by entity $a$ and $D(t)$ to denote the set of messages
potentially affected by event $t$ (e.g., all messages sent after time
interval $t$, in the case of an exponential decay function). The
conditional distributions are:
\begin{align}
(\mathbold{\beta}_k \g \mathbf{N}, \mathbold{\beta}, \mathbold{\eta}, \mathbold{\gamma}, \mathbold{\theta}, \mathbold{\zeta}, \mathbold{\epsilon}, \mathbold{\phi}, \mathbold{\xi}, \mathbold{\psi})
&\sim \textrm{Dirichlet}_V\left(\alpha + \sum_{d=1}^D, \ldots, \alpha
+ \ldots\right)\\
\end{align}

$\mathbf{N}, \mathbold{\beta}, \mathbold{\eta}, \mathbold{\gamma}, \mathbold{\theta}, \mathbold{\zeta}, \mathbold{\epsilon}, \mathbold{\phi}, \mathbold{\xi},
\mathbold{\psi}$

\begin{equation}
\mathbold{\gamma}_t \g \mathbf{N}, \mathbold{\psi}, \mathbold{\phi}, \mathbold{\beta}, \mathbold{\xi}, \mathbold{\eta}, \mathbold{\theta}, \mathbold{\epsilon}, \mathbold{\zeta}, \mathbf{z} \sim
	\mbox{Dirichlet}_V\left(\alpha + \sum_{d=1}^D
		z^\mathcal{T}_{d1t}, \dots, \alpha + \sum_{d=1}^D z^\mathcal{T}_{dVt}
	\right)
\label{eq:pi}
\end{equation}
\begin{equation}
\mathbold{\eta}_n \g \mathbf{N}, \mathbold{\psi}, \mathbold{\gamma}, \mathbold{\phi}, \mathbold{\beta}, \mathbold{\xi}, \mathbold{\theta}, \mathbold{\epsilon}, \mathbold{\zeta}, \mathbf{z} \sim
	\mbox{Dirichlet}_V\left(\alpha + \sum_{d\in D(n)}
		z^\mathcal{A}_{dv}, \dots, \alpha + \sum_{d\in D(n)} z^\mathcal{A}_{dv}
	\right)
\label{eq:eta}
\end{equation}
\begin{equation}
\mathbold{\beta}_k \g \mathbf{N}, \mathbold{\psi}, \mathbold{\gamma}, \mathbold{\phi}, \mathbold{\xi}, \mathbold{\eta}, \mathbold{\theta}, \mathbold{\epsilon}, \mathbold{\zeta}, \mathbf{z} \sim
	\mbox{Dirichlet}_V\left(\alpha + \sum_{d=1}^D
		z^\mathcal{K}_{d1k}, \dots, \alpha + \sum_{d=1}^D z^\mathcal{K}_{dVk}
	\right)
\label{eq:beta}
\end{equation}
\begin{equation}
\psi_t \g \mathbf{N}, \mathbold{\gamma}, \mathbold{\phi}, \mathbold{\beta}, \mathbold{\xi}, \mathbold{\eta}, \mathbold{\theta}, \mathbold{\epsilon}, \mathbold{\zeta}, \mathbf{z} \sim
	\mbox{Gamma}\left(
		s + |D(t)| s,
		r + \sum_{d\in D(t)} \epsilon_{dt}
	\right)
\label{eq:psi}
\end{equation}
\begin{equation}
\xi_n \g \mathbf{N}, \mathbold{\psi}, \mathbold{\gamma}, \mathbold{\phi}, \mathbold{\beta}, \mathbold{\eta}, \mathbold{\theta}, \mathbold{\epsilon}, \mathbold{\zeta}, \mathbf{z} \sim
	\mbox{Gamma}\left(
		s + |D(n)| s,
		r + \sum_{d\in D(n)} \zeta_{d}
	\right)
\label{eq:xi}
\end{equation}
\begin{equation}
\phi_{nk} \g \mathbf{N}, \mathbold{\psi}, \mathbold{\gamma}, \mathbold{\beta}, \mathbold{\xi}, \mathbold{\eta}, \mathbold{\theta}, \mathbold{\epsilon}, \mathbold{\zeta}, \mathbf{z} \sim
	\mbox{Gamma}\left(
		s + |D(n)| s,
		r + \sum_{d\in D(n)} \theta_{dk}
	\right)
\label{eq:phi}
\end{equation}
\begin{equation}
\theta_{dk} \g \mathbf{N}, \mathbold{\psi}, \mathbold{\gamma}, \mathbold{\phi}, \mathbold{\beta}, \mathbold{\xi}, \mathbold{\eta}, \mathbold{\epsilon}, \mathbold{\zeta}, \mathbf{z} \sim
	\mbox{Gamma}\left(
		s + \sum_{v=1}^V z^\mathcal{K}_{dvk},
		\phi_{a_d k} + \sum_{v=1}^V \beta_{kv}
	\right)
\label{eq:theta}
\end{equation}
\begin{equation}
\epsilon_{dt} \g \mathbf{N}, \mathbold{\psi}, \mathbold{\gamma}, \mathbold{\phi}, \mathbold{\beta}, \mathbold{\xi}, \mathbold{\eta}, \mathbold{\theta}, \mathbold{\zeta}, \mathbf{z} \sim
	\mbox{Gamma}\left(
		s + \sum_{v=1}^V z^\mathcal{T}_{dvt},
		\psi_t + f(t_d, t) \sum_{v=1}^V \gamma_{tv}
	\right)
\label{eq:epsilon}
\end{equation}
\begin{equation}
\zeta_{d} \g \mathbold{\psi}, \mathbold{\gamma}, \mathbold{\phi}, \mathbold{\beta}, \mathbold{\xi}, \mathbold{\eta}, \mathbold{\theta}, \mathbold{\epsilon}, \mathbf{z} \sim
	\mbox{Gamma}\left(
		s + \sum_{v=1}^V z^\mathcal{A}_{dv},
		\xi_{a_d} + \sum_{v=1}^V \eta_{a_d v}
	\right)
\label{eq:zeta}
\end{equation}

The complete conditional for the auxiliary variables has the form
\[\mathbf{z}_{dv} \g \mathbf{N}, \mathbold{\psi}, \mathbold{\gamma}, \mathbold{\phi}, \mathbold{\beta}, \mathbold{\xi}, \mathbold{\eta}, \mathbold{\theta}, \mathbold{\epsilon}, \mathbold{\zeta} \sim \mbox{Mult}(n_{dv}, \mathbold{\omega}_{dv}),\] where
\begin{equation}
\mathbold{\omega}_{dv} \propto \langle
\theta_{d1} \beta_{1v}, \dots, \theta_{dK} \beta_{Kv}, \zeta_d \eta_{a_d v},
f(t_d, 1) \epsilon_{d1} \gamma_{1v}, \dots, f(t_d, T) \epsilon_{dT} \gamma_{Tv}\rangle.
\label{eq:omega}
\end{equation}
Intuitively, these variables allocate the data to one of the entity concerns or events, and thus can be used to explore the data.

Given these conditionals, the algorithm sets each parameter to the expected conditional parameter under the variational distribution. The mean field assumption guarantees that this expectation will not involve the parameter being updated.  \Cref{alg:capsule} shows our variational inference algorithm.

This algorithm uses the notation $\mathbold{\lambda}$ to refer to the set of variational parameters,
\[\mathbold{\lambda} = \{\lambda^\gamma, \lambda^\eta, \lambda^\beta, \lambda^\psi, \lambda^\zeta, \lambda^\phi, \lambda^\theta, \lambda^\epsilon, \lambda^\xi\}.\] The notation $V(d)$ is the set of vocabulary indices for the collection of words in document $d$.  We could also iterate over all $V$, but as zero word counts give $\E[\mathbf{z}_{dv}] = 0~\forall v \not\in V(d)$, the two are equivalent.

%TODO: check for consistent indexing of variables (e.g., beta_{v,k} vs beta_{k,v})


\begin{algorithm}[p]
\small
\DontPrintSemicolon
\KwIn{word counts $w$}
%($\psi$, $\gamma$, $\xi$, $\eta$, $\phi$, $\beta$, $\theta$, $\zeta$, $\epsilon$)
\KwOut{approximate posterior of latent parameters in terms of variational parameters $\mathbold{\lambda}$}
\textbf{Initialize} $\E[\mathbold{\beta}_k]$ to slightly random around uniform for each $k$\;
\textbf{Initialize} $\E[\mbox{all other parameters}]$ to uniform \;
\For {iteration $m=1, dots, M$}{
	\textbf{set} all $\mathbold{\lambda}$ to respective priors, excluding $\lambda^{\theta,rate}$, $\lambda^{\zeta,rate}$, and $\lambda^{\epsilon,rate}$, which are set to 0 \;
	\textbf{update} $\lambda^{\theta,rate}_{dk} \pluseq \sum_V \E[\mathbold{\beta}_{kv}]$  for all messages $d$ and topics $k$\;
	\For {each message $d=1, \dots, D$}{
		\For {each term $v\in V(d)$\footnotemark}{
			\textbf{set} $(K+T+1)$-vector $\mathbold{\omega}_{dv}$ as shown in \cref{eq:omega}, using $\E$ of parameters\;
			\textbf{set} $(K+T)$-vector $\E[\mathbf{z}_{dv}] = n_{dv} * \mathbold{\omega}_{dv}$ \;
			\textbf{update} $\lambda^{\theta,shape}_{dk} \pluseq \E[z^\mathcal{K}_{dvk}]$ for all $k$ [\cref{eq:theta}]\;
			\textbf{update} $\lambda^{\epsilon,shape}_{dt} \pluseq \E[z^\mathcal{K}_{dvt}]$ for all $t$ [\cref{eq:epsilon}]\;
			\textbf{update} $\lambda^{\zeta,shape}_d \pluseq \E[z^\mathcal{A}_{dv}]$ [\cref{eq:zeta}]\;

			\textbf{update} $\lambda^{\beta}_{kv} \pluseq \E[z^\mathcal{K}_{dvk}]$ for all $k$ [\cref{eq:beta}]\;
			\textbf{update} $\lambda^{\gamma}_{tv} \pluseq \E[z^\mathcal{T}_{dvt}]$ for all $t$ [\cref{eq:pi}]\;
			\textbf{update} $\lambda^{\eta}_{a_d v} \pluseq \E[z^\mathcal{A}_{dv}]$ [\cref{eq:eta}]\;
		}

		\textbf{set} $\lambda^{\theta,rate}_{dk} = \E[\phi_{a_d k}] +\sum_v \E[\beta_{kv}]$ for all $k$ [\cref{eq:theta}]\;
		\textbf{set} $\lambda^{\epsilon,rate}_{dt} = \E[\psi_t] + f\sum_v \E[\gamma_{tv}]$ for all $t$ [\cref{eq:epsilon}]\;
		\textbf{set} $\lambda^{\zeta,rate}_{d} = \E[\xi_{a_d}] + \sum_v\E[\eta_{a_d v}]$ [\cref{eq:zeta}]\;

		\textbf{set} $\E[\theta_{dk}] = \lambda^{\theta,shape}_{dk} / \lambda^{\theta,rate}_{dk}$ for all $k$\;
		\textbf{set} $\E[\epsilon_{dt}] = \lambda^{\epsilon,shape}_{dt} / \lambda^{\epsilon,rate}_{dt}$ for all $t$\;
		\textbf{set} $\E[\zeta_d] = \lambda^{\zeta,shape}_d / \lambda^{\zeta,rate}_d$ \;

		\textbf{update} $\lambda^{\phi,shape}_{a_d k} \pluseq s$ for all $k$ [\cref{eq:phi}] \;
		\textbf{update} $\lambda^{\psi,shape}_{t} \pluseq s \forall t : f(t_d, t) \neq 0$ [\cref{eq:psi}] \;
		\textbf{update} $\lambda^{\xi,shape}_{a_d} \pluseq s$ [\cref{eq:xi}] \;
		\textbf{update} $\lambda^{\phi,rate}_{a_d k} \pluseq \theta_{dk}$ for all $k$ [\cref{eq:phi}] \;
		\textbf{update} $\lambda^{\psi,rate}_t \pluseq \epsilon_{dt}$ for all $t$ [\cref{eq:psi}] \;
		\textbf{update} $\lambda^{\xi,rate}_{a_d} \pluseq \zeta_d$ [\cref{eq:xi}] \;

	}

	\textbf{set} $\E[\phi_{ak}] = \lambda^{\phi,shape}_{ak} / \lambda^{\phi,rate}_{ak}$ for all $a$ and $k$ \;
	\textbf{set} $\E[\mathbold{\beta}_{k}] = \mathbold{\lambda}^{\beta}_{k} / \sum_v \lambda^{\beta}_{kv}$ for all $k$ \;
	\textbf{set} $\E[\xi_{d}] = \lambda^{\xi,shape}_d / \lambda^{\xi,rate}_d$ for all $d$ \;
	\textbf{set} $\E[\mathbold{\eta}_{a}] = \mathbold{\lambda}^{\eta}_{a} / \sum_v \lambda^{\eta}_{nv}$ for all $a$ \;
	\textbf{set} $\E[\psi_a] = \lambda^{\psi,shape}_a / \lambda^{\psi,rate}_a$ for all $a$\;
	\textbf{set} $\E[\mathbold{\gamma}_t] = \mathbold{\lambda}^{\gamma}_t / \sum_v \lambda^{\gamma}_{tv}$ for all $t$ \;
}
\Return{$\mathbold{\lambda}$} \;
\caption{Variational Inference for Capsule}
\label{alg:capsule}
\end{algorithm}

This algorithm produces a fitted variational distribution which can then be used as a proxy for the true posterior, allowing us to explore a collection of documents with Capsule.  Source code is available at \url{https://github.com/ajbc/capsule}.
