%!TEX root = supplemental.tex

In this appendix, we describe the details of the approximate inference
algorithm for Capsule.

Conditioned on the observed term counts---$n_{dv}$ for vocabulary term
$v$ in message $d$; collectively $\mathbf{N}$---our goal is to learn
the posterior distribution of the latent variables. Each message is
associated with an author entity $a_d$ and a time interval $t_d$
within which that messages was sent. The latent variables are the
general topics $\mathbold{\beta}_1, \ldots, \mathbold{\beta}_K$, the
entity topics $\mathbold{\eta}_1, \ldots, \mathbold{\eta}_A$, and the
event topics $\mathbold{\gamma}_1, \ldots, \mathbold{\gamma}_T$, as
well as the message-specific strengths
$\mathbold{\theta}_{1}, \ldots, \mathbold{\theta}_D$,
$\zeta_{1}, \ldots, \zeta_D$, and
$\mathbold{\epsilon}_{1}, \ldots, \mathbold{\epsilon}_D$, the
entity-specific strengths
$\mathbold{\phi}_1, \ldots, \mathbold{\phi}_A$ and
$\xi_1, \ldots, \xi_A$, and the event strengths
$\psi_1, \ldots, \psi_T$. See \cref{fig:graphicalmodel,fig:generative-model}
for the graphical model and generative process.

As for many Bayesian models, the posterior distribution is not
tractable to compute; we must instead approximate it. We therefore
introduce an approximate inference algorithm for Capsule, based on
variational methods~\cite{Jordan:1999,Wainwright:2008}. Variational
methods approximate the true posterior distribution $p$ with a
(simpler) variational distribution $q$. Inference then consists of
minimizing the KL divergence from $q$ to $p$. This is equivalent to
maximizing the evidence lower bound (ELBO):
\begin{equation}
\mathcal{L}(q)  = \E_{q}\left[\log{
p(\mathbf{N}, \mathbold{\beta}, \mathbold{\eta}, \mathbold{\gamma}, \mathbold{\theta}, \mathbold{\zeta}, \mathbold{\epsilon}, \mathbold{\phi}, \mathbold{\xi},
\mathbold{\psi})}
- \log{q(\mathbold{\beta}, \mathbold{\eta}, \mathbold{\gamma}, \mathbold{\theta}, \mathbold{\zeta}, \mathbold{\epsilon}, \mathbold{\phi}, \mathbold{\xi},
\mathbold{\psi})}\right].  \label{eq:cables:elbo}
\end{equation}

We define $q$ using the mean field assumption:
\begin{align}
&q(\mathbold{\beta}, \mathbold{\eta}, \mathbold{\gamma}, \mathbold{\theta}, \mathbold{\zeta}, \mathbold{\epsilon}, \mathbold{\phi}, \mathbold{\xi},
\mathbold{\psi})
= \notag\\
&\quad
        	 	\prod_{d=1}^D \left(q(\zeta_d \g \lambda_d)
				\prod_{k=1}^K q(\theta_{dk} \g \lambda^\theta_{dk})
				\prod_{t=1}^T q(\epsilon_{dt} \g \lambda^\epsilon_{dt})
			\right) \times{}\notag \\
&\quad \prod_{k=1}^K \left(
q(\mathbold{\beta}_k \g \lambda_k^{\beta}) \prod_{a=1}^A
q(\phi_{ak} \g \lambda_{ak}^{\phi})\right) \prod_{a=1}^A \left(q(\mathbold{\eta}_a
\g \lambda_a^{\eta})\,q(\xi_a \g \lambda_a^{\xi}) \right) \prod_{t=1}^T \left(
q(\mathbold{\gamma}_t \g \lambda_t^{\gamma})\,q(\psi_t \g \lambda_t^{\gamma}\right)
	\label{eq:q}
\end{align}

The variational distributions for the topics $q(\mathbold{\beta}_k)$,
$q(\mathbold{\eta}_a)$, and $q(\mathbold{\gamma}_t)$ are all Dirichlet
distributions with free variational parameters $\lambda_{k}^{\beta}$,
$\lambda_a^{\eta}$, and $\lambda_t^{\gamma}$, respectively. The
variational distributions for the strengths $q(\theta_{dk})$,
$q(\zeta_{d})$, $q(\epsilon_{dt})$, $q(\phi_{ak})$, $q(\xi_a)$, and
$q(\psi_t)$ are all gamma distributions with free variational
parameters $\lambda_{dk}^{\theta}$, $\lambda_d^{\zeta}$,
$\lambda_{dt}^{\epsilon}$, $\lambda_{ak}^{\phi}$, $\lambda_a^{\xi}$,
and $\lambda_{t}^{\psi}$, respectively. Each of these parameters has
two components: shape $s$ and rate $r$.

The expectations under $q$, which we need to maximize the ELBO, have
closed analytic forms. We therefore update each free variational
parameter in turn, following a standard coordinate-ascent approach.

To obtain update equations for the free variational parameters, we
introduce auxiliary latent variables:
\begin{align}
z_{dkv}^{\mathcal{K}}
&\sim \textrm{Poisson}\left(\theta_{dk}\beta_{kv}\right)\\
z_{dv}^{\mathcal{A}}
&\sim \textrm{Poisson}\left(\zeta_{d}\eta_{a_d v}\right)\\
z_{dtv}^{\mathcal{T}}
&\sim \textrm{Poisson}\left(f(t_d, t)\,\epsilon_{d t} \gamma_{tv}\right),
\end{align}
where the superscripts $\mathcal{K}$, $\mathcal{A}$, and $\mathcal{T}$
indicate the general, entity, and event topics, respectively. When
marginalized out, these variables---collectively $\mathbf{z}$---leave the model
intact. Because the Poisson distribution has an additive
property, the value of $n_{dv}$ is completely determined by the values
of these variables:
\begin{equation}
n_{dv} = \sum_{k=1}^K z_{dkv}^{\mathcal{K}} + z_{dv}^{\mathcal{A}}
+ \sum_{t=1}^T z_{dtv}^{\mathcal{T}}.
\end{equation}

Coordinate-ascent variational inference depends on the conditional
distribution of each latent variable given the values of the other
latent variables and the data. We use $D(a)$ to denote the set of
messages sent by entity $a$ and $D(t)$ to denote the set of messages
potentially affected by event $t$ (e.g., all messages sent after time
interval $t$, in the case of an exponential decay function). The
conditional distributions are:
\begin{align}
\label{eq:beta}
(\mathbold{\beta}_k \g \mathbf{N}, \mathbf{z}, \mathbold{\eta}, \mathbold{\gamma}, \mathbold{\theta}, \mathbold{\zeta}, \mathbold{\epsilon}, \mathbold{\phi}, \mathbold{\xi}, \mathbold{\psi})
&\sim \textrm{Dirichlet}_V\left(\alpha + \sum_{d=1}^D z_{dk1}^{\mathcal{K}}, \ldots, \alpha
+ \sum_{d=1}^D z_{dkV}^{\mathcal{K}}\right)\\
\label{eq:eta}
(\mathbold{\eta}_a \g \mathbf{N}, \mathbf{z}, \mathbold{\beta}, \mathbold{\gamma}, \mathbold{\theta}, \mathbold{\zeta}, \mathbold{\epsilon}, \mathbold{\phi}, \mathbold{\xi}, \mathbold{\psi})
&\sim \textrm{Dirichlet}_V\left(\alpha + \sum_{d \in D(a)}
z_{d1}^{\mathcal{A}}, \ldots, \alpha + \sum_{d \in D(a)}
z_{dV}^{\mathcal{A}}\right)\\
\label{eq:pi}
(\mathbold{\gamma}_t \g \mathbf{N}, \mathbf{z}, \mathbold{\beta}, \mathbold{\eta}, \mathbold{\theta}, \mathbold{\zeta}, \mathbold{\epsilon}, \mathbold{\phi}, \mathbold{\xi}, \mathbold{\psi})
&\sim \textrm{Dirichlet}_V\left(\alpha + \sum_{d \in D(t)}
z_{d1t}^{\mathcal{T}}, \ldots, \alpha + \sum_{d \in D(t)}
z_{dVt}^{\mathcal{T}}\right)\\
% should this really be a sum over D rather than D(t)?
\label{eq:theta}
(\theta_{dk} \g \mathbf{N}, \mathbf{z}, \mathbold{\beta}, \mathbold{\eta}, \mathbold{\gamma}, \mathbold{\zeta}, \mathbold{\epsilon}, \mathbold{\phi}, \mathbold{\xi}, \mathbold{\psi})
&\sim \textrm{Gamma}\left(s + \sum_{v=1}^V
z_{dkv}^{\mathcal{K}},\, \phi_{a_d k}
+ \sum_{v=1}^V \beta_{kv} \right)\\
\label{eq:zeta}
(\zeta_d \g \mathbf{N}, \mathbf{z}, \mathbold{\beta}, \mathbold{\eta}, \mathbold{\gamma}, \mathbold{\theta}, \mathbold{\epsilon}, \mathbold{\phi}, \mathbold{\xi}, \mathbold{\psi})
&\sim \textrm{Gamma}\left(s + \sum_{v=1}^V
z_{dv}^{\mathcal{A}},\, \xi_{a_d} + \sum_{v=1}^V \eta_{a_d
v} \right)\\
\label{eq:epsilon}
(\epsilon_{dt} \g \mathbf{N}, \mathbf{z}, \mathbold{\beta}, \mathbold{\eta}, \mathbold{\gamma}, \mathbold{\theta}, \mathbold{\zeta}, \mathbold{\phi}, \mathbold{\xi}, \mathbold{\psi})
&\sim \textrm{Gamma}\left(s + \sum_{v=1}^V
z_{dtv}^{\mathcal{T}},\, \psi_t + f(t_d,
t)\sum_{v=1}^V \gamma_{tv} \right)\\
\label{eq:phi}
(\phi_{ak} \g \mathbf{N}, \mathbf{z}, \mathbold{\beta}, \mathbold{\eta}, \mathbold{\gamma}, \mathbold{\theta}, \mathbold{\zeta}, \mathbold{\epsilon}, \mathbold{\xi}, \mathbold{\psi})
&\sim \textrm{Gamma}\left(s + \lvert D(a) \rvert\,s,\,r + \sum_{d \in
D(a)} \theta_{dk} \right)\\
\label{eq:xi}
(\xi_a \g \mathbf{N}, \mathbf{z}, \mathbold{\beta}, \mathbold{\eta}, \mathbold{\gamma}, \mathbold{\theta}, \mathbold{\zeta}, \mathbold{\epsilon}, \mathbold{\phi}, \mathbold{\psi})
&\sim \textrm{Gamma}\left(s + \lvert D(a) \rvert\,s,\, r + \sum_{d \in
D(a)} \zeta_d \right)\\
\label{eq:psi}
(\psi_t \g \mathbf{N}, \mathbf{z}, \mathbold{\beta}, \mathbold{\eta}, \mathbold{\gamma}, \mathbold{\theta}, \mathbold{\zeta}, \mathbold{\epsilon}, \mathbold{\phi}, \mathbold{\xi})
&\sim \textrm{Gamma}\left(s + \lvert D(t) \rvert\,s,\,r + \sum_{d \in
D(t)} \epsilon_{dt} \right).
\end{align}

%\mathbf{N}, \mathbf{z}, \mathbold{\beta}, \mathbold{\eta}, \mathbold{\gamma}, \mathbold{\theta}, \mathbold{\zeta}, \mathbold{\epsilon}, \mathbold{\phi}, \mathbold{\xi}, \mathbold{\psi}

\pagebreak

The conditional distribution of the auxiliary latent variables is:
\begin{equation}
(\langle \mathbf{z}_{dv}^{\mathcal{K}}, \mathbf{z}_{dv}^{\mathcal{A}}, \mathbf{z}_{dv}^{\mathcal{T}}\rangle \g \mathbf{N}, \mathbold{\beta}, \mathbold{\eta}, \mathbold{\gamma}, \mathbold{\theta}, \mathbold{\zeta}, \mathbold{\epsilon}, \mathbold{\phi}, \mathbold{\xi}, \mathbold{\psi}) \sim \textrm{Mult}\left(n_{dv}, \mathbold{\omega}_{dv} \right),
\end{equation}
where
\begin{equation}
\mathbold{\omega}_{dv}
\propto \langle \theta_{d1} \beta_{1v}, \ldots, \theta_{dK} \beta_{Kv}, \zeta_d \eta_{a_d
v}, f(t_d, 1)\,\epsilon_{d1}\gamma_{1v}, \ldots, f(t_d,
T)\,\epsilon_{dT}\gamma_{Tv} \rangle.
\label{eq:omega}
\end{equation}

Given the conditional distributions, coordinate-ascent variational
inference involves setting each free variational parameter to the
expected value of the corresponding model parameter under the
variational distribution. We provide pseudeocode
in \cref{alg:capsule}; we use $\mathbold{\lambda}$ to denote the
entire set of free variational parameters and $V(d)$ to denote the set
of vocabulary terms present in document $d$. Our approximate inference
algorithm produces a fitted variational posterior distribution which
can then be used as a proxy for the true posterior distribution. The
source code is available online at \url{https://github.com/ajbc/capsule}.

%We could also iterate over all $V$, but as zero word counts give $\E[\mathbf{z}_{dv}] = 0~\forall v \not\in V(d)$, the two are equivalent.

%TODO: check for consistent indexing of variables (e.g., beta_{v,k} vs beta_{k,v})


\begin{algorithm}[p]
\small
\DontPrintSemicolon
\KwIn{observed term counts $\mathbf{N}$}
\KwOut{approximate posterior distribution of the latent variables, in terms of free variational parameters $\mathbold{\lambda}$}
\textbf{Initialize} $\E[\mathbold{\beta}_k]$ to slightly random around
uniform for each general topic $k$\;
\textbf{Initialize} $\E[\textrm{\,all other latent variables\,}]$ to uniform \;
\For {\textrm{iteration} $m=1, \dots, M$}{
	\textbf{set}  $\lambda^{\theta,r}$, $\lambda^{\zeta,r}$, and
	$\lambda^{\epsilon,r}$ to 0 and set remaining $\mathbold{\lambda}$ using priors\;
	\textbf{update}
	$\lambda^{\theta,r}_{dk} \pluseq \sum_V \E[\mathbold{\beta}_{kv}]$
	for each message $d$ and general topic $k$\;
	\For {\textrm{message} $d=1, \dots, D$}{
		\For {\textrm{term} $v\in V(d)$}{
			\textbf{set} $\mathbold{\omega}_{dv}$ using expected
	values of the latent variables (\cref{eq:omega})\;
			\textbf{set}
	$\E[\langle \mathbf{z}_{dv}^{\mathcal{K}}, \mathbf{z}_{dv}^{\mathcal{A}},
	\mathbf{z}_{dv}^{\mathcal{T}} \rangle] = n_{dv} \mathbold{\omega}_{dv}$ \;
			\textbf{update}
	$\lambda^{\beta}_{kv} \pluseq \E[z^\mathcal{K}_{dkv}]$ for
	each general topic $k$ (\cref{eq:beta})\;
			\textbf{update} $\lambda^{\eta}_{a_d
	v} \pluseq \E[z^\mathcal{A}_{dv}]$ (\cref{eq:eta})\;
			\textbf{update}
	$\lambda^{\gamma}_{tv} \pluseq \E[z^\mathcal{T}_{dtv}]$ for
	each time interval $t$ (\cref{eq:pi})\;
	\textbf{update}
	$\lambda^{\theta,s}_{dk} \pluseq \E[z^\mathcal{K}_{dkv}]$ for
	each general topic $k$ (\cref{eq:theta})\;
			\textbf{update}
	$\lambda^{\zeta,s}_d \pluseq \E[z^\mathcal{A}_{dv}]$ (\cref{eq:zeta})\;
			\textbf{update}
	$\lambda^{\epsilon,s}_{dt} \pluseq \E[z^\mathcal{K}_{dtv}]$
	for each time interval $t$ (\cref{eq:epsilon})\;
		}

		\textbf{set} $\lambda^{\theta,r}_{dk} = \E[\phi_{a_d
		k}] +\sum_v \E[\beta_{kv}]$ for each general topic $k$
		(\cref{eq:theta})\;
		\textbf{set} $\lambda^{\zeta,r}_{d} = \E[\xi_{a_d}]
		+ \sum_v\E[\eta_{a_d v}]$  (\cref{eq:zeta})\;
		\textbf{set} $\lambda^{\epsilon,r}_{dt} = \E[\psi_t] +
		f(t_d, t) \sum_v \E[\gamma_{tv}]$ for each time
		interval $t$ (\cref{eq:epsilon})\;

		\textbf{set} $\E[\theta_{dk}]
		= \lambda^{\theta,s}_{dk}
		\,/\, \lambda^{\theta,r}_{dk}$ for each general topic $k$\;
		\textbf{set} $\E[\zeta_d] = \lambda^{\zeta,s}_d \,/\, \lambda^{\zeta,r}_d$ \;
		\textbf{set} $\E[\epsilon_{dt}]
		= \lambda^{\epsilon,s}_{dt}
		\,/\, \lambda^{\epsilon,r}_{dt}$ for each time interval $t$\;


		\textbf{update} $\lambda^{\phi,s}_{a_d k} \pluseq s$
		for each general topic $k$ (\cref{eq:phi}) \;
		\textbf{update} $\lambda^{\xi,s}_{a_d} \pluseq s$
		(\cref{eq:xi})\;
		\textbf{update} $\lambda^{\psi,s}_{t} \pluseq
		s$ for each time interval $t$ where $f(t_d, t) \neq 0$
		(\cref{eq:psi}) \;

		\textbf{update} $\lambda^{\phi,r}_{a_d
		k} \pluseq \theta_{dk}$ for each general topic $k$ (\cref{eq:phi}) \;
		\textbf{update}
		$\lambda^{\xi,r}_{a_d} \pluseq \zeta_d$
		(\cref{eq:xi}) \;
		\textbf{update}
		$\lambda^{\psi,r}_t \pluseq \epsilon_{dt}$ for each
		time interval $t$ (\cref{eq:psi}) \;

	}

	\textbf{set} $\E[\mathbold{\beta}_{k}]
	= \mathbold{\lambda}^{\beta}_{k} \,/\, \sum_v \lambda^{\beta}_{kv}$
	for each general topic $k$ \;

	\textbf{set} $\E[\mathbold{\eta}_{a}]
	= \mathbold{\lambda}^{\eta}_{a} \,/\, \sum_v \lambda^{\eta}_{av}$
	for each entity $a$ \;

	\textbf{set} $\E[\mathbold{\gamma}_t]
	= \mathbold{\lambda}^{\gamma}_t \,/\, \sum_v \lambda^{\gamma}_{tv}$
	for each time interval $t$ \;

	\textbf{set} $\E[\phi_{ak}] = \lambda^{\phi,s}_{ak}
	\,/\, \lambda^{\phi,r}_{ak}$ for each entity $a$ and general
	topic $k$ \;

	\textbf{set} $\E[\xi_{a}] = \lambda^{\xi,s}_a
	\,/\, \lambda^{\xi,r}_a$ for each entity $a$\;

	\textbf{set} $\E[\psi_t]
	= \lambda^{\psi,s}_t \,/\, \lambda^{\psi,r}_t$ for each time interval $t$\;

}
\Return{$\mathbold{\lambda}$} \;
\caption{Coordinate-ascent variational inference for Capsule.}
\label{alg:capsule}
\end{algorithm}
