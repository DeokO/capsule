%!TEX root = emnlp2016.tex

In this appendix, we describe the details of the variational inference algorithm for Capsule. This algorithm fits the parameters of the variational distribution $q$ in Eq.~\ref{eq:q} so that it is close in KL divergence to the posterior.

Recall that the variational distributions $q(\pi)$ and $q(\beta)$ are both Dirichlet-distributed with free variational parameters $\lambda^\pi$ and $\lambda^\beta$, respectively.  Similarly, the variational distributions $q(\psi)$, $q(\phi)$, $q(\theta)$ and $q(\epsilon)$ are all gamma-distributed with corresponding free variational parameters $\lambda^\psi$, $\lambda^\phi$, $\lambda^\theta$, and $\lambda^\epsilon$.  For these gamma-distributed variables, each free parameter $\lambda$ has two components: shape $s$ and rate $r$.

Minimizing the KL divergence between the true posterior $p$ and the variational approximation $q$ is equivalent to maximizing the ELBO (Eq.~\ref{eq:elbo}).  We achieve this with closed form coordinate updates, as the Capsule model is specified with the required conjugate relationships that make this approach possible~\cite{Ghahramani:2001}.

To obtain simple updates, we first rely on auxiliary latent variables $z$. These variables, when marginalized out, leave the original model intact. The Poisson distribution has an additive property; specifically if $w \sim \mbox{Poisson}(a+b)$, then $w = z_1 + z_2$, where $z_1 \sim \mbox{Poisson}(z_1)$ and $z_2 \sim \mbox{Poisson}(z_2)$.  We apply this decomposition to the word count distribution in Eq.~\ref{} and define Poisson variables for each component of the word count:
\begin{equation*}
  z^\mathcal{K}_{d,v,k} \sim \mbox{Poisson}(\theta_{d,k}\beta_{k,v}) \quad
  z^\mathcal{T}_{d,v,t} \sim \mbox{Poisson}\left(f(i_d, t) \epsilon_{d,t} \pi_{t,v}\right).
\end{equation*}
The $\mathcal{K}$ and $\mathcal{T}$ superscripts indicate the contributions from entity concerns and events, respectively.  Given these variables, the total word count is deterministic:
\[ w_{d,v} = \sum_{k=1}^K z^\mathcal{K}_{d,v,k} + \sum_{t=1}^T z^\mathcal{T}_{d,v,t}. \]

TODO: text here....

\begin{multline}
\pi_t \g \mathbf{W}, \psi, \phi, \beta, \theta, \epsilon, z \sim \\
	\mbox{Dirichlet}_V\left(\alpha_\pi + \sum_{d=1}^D \langle
		z^\mathcal{T}_{d,1,t}, \cdots, z^\mathcal{T}_{d,V,t}\rangle
	\right)
\label{eq:pi}
\end{multline}

\begin{multline}
\beta_k \g \mathbf{W}, \psi, \pi, \phi, \theta, \epsilon, z \sim \\
	\mbox{Dirichlet}_V\left(\alpha_\beta + \sum_{d=1}^D \langle
		z^\mathcal{K}_{d,1,k}, \cdots, z^\mathcal{K}_{d,V,k}\rangle
	\right)
\label{eq:beta}
\end{multline}

\begin{multline}
\psi_t \g \mathbf{W}, \pi, \phi, \beta, \theta, \epsilon, z \sim \\
	\mbox{Gamma}\left(
		s_\psi + |D(t)| s_\epsilon,
		r_\psi + \sum_{d\in D(t)} \epsilon_{d,t}
	\right)
\label{eq:psi}
\end{multline}

\begin{multline}
\phi_{i,k} \g \mathbf{W}, \psi, \pi, \beta, \theta, \epsilon, z \sim \\
	\mbox{Gamma}\left(
		s_\phi + |D(i)| s_\theta,
		r_\phi + \sum_{d\in D(i)} \theta_{d,k}
	\right)
\label{eq:phi}
\end{multline}

\begin{multline}
\theta_{d,k} \g \mathbf{W}, \psi, \pi, \phi \beta, \epsilon, z \sim \\
	\mbox{Gamma}\left(
		s_\theta + \sum_{v=1}^V z^\mathcal{K}_{d,v,k},
		\phi_{a_d,k} + \sum_{v=1}^V \beta_{k,v}
	\right)
\label{eq:theta}
\end{multline}

\begin{multline}
\epsilon_{d,t} \g \mathbf{W}, \psi, \pi, \phi, \beta, \theta, z \sim \\
	\mbox{Gamma}\left(
		s_\epsilon + \sum_{v=1}^V z^\mathcal{T}_{d,v,t},
		\psi_t + f(i_d, t) \sum_{v=1}^V \pi_{t,v}
	\right)
\label{eq:epsilon}
\end{multline}

Where $D(i)$ is the set of documents sent by entity $i$ and $D(t)$ is the set of documents sent impacted by events at time $t$ (e.g., all documents after the event in the case of exponential decay).

$z_{d,v} \g \psi, \pi, \phi, \beta, \theta, \epsilon \sim \mbox{Mult}(w_{d,v}, \omega_{d,v})$ where
\begin{multline}
\omega_{d,v} \propto \langle 
\theta_{d,1} \beta_{1,v}, \cdots, \theta_{d,K} \beta_{K,v}, \\
f(i_d, 1) \epsilon_{d,1} \pi_{1,v}, \cdots, f(i_d, T) \epsilon_{d,T} \pi_{T,v}\rangle.
\label{eq:omega}
\end{multline}

TODO: check for consistent indexing of variables (e.g., beta_{v,k} vs beta_{k,v})


\begin{algorithm}
\small
\DontPrintSemicolon
\KwIn{word counts $w$}
\KwOut{approximate posterior of latent parameters ($\psi$, $\pi$, $\phi$, $\beta$, $\theta$, $\epsilon$) in terms of variational parameters $\lambda$ = $\{\lambda^\psi$, $\lambda^\pi$, $\lambda^\phi$, $\lambda^\beta$, $\lambda^\theta$, $\lambda^\epsilon\}$}
\textbf{Initialize} $\E[\beta]$ to slightly random around uniform \;
\textbf{Initialize} $\E[\psi]$, $\E[\pi]$, $\E[\psi]$, $\E[\theta]$, $\E[\epsilon]$ to uniform \;
\For {iteration $m=1:M$}{
	\textbf{set} $\lambda^\psi$, $\lambda^\pi$, $\lambda^\phi$, $\lambda^\beta$, $\lambda^\theta$, $\lambda^\epsilon$ to respective priors, excluding $\lambda^{\theta,rate}$ and $\lambda^{\epsilon,rate}$, which are set to 0 \;
	\textbf{update} $\lambda^{\theta,rate} \pluseq \sum_V \E[\beta_v]$ \;
	\For {each document $d=1:D$}{
		\For {each term $v\in V(d)$\footnotemark}{
			\textbf{set} $(K+T)$-vector $\omega_{d,v}$ using $\E[\pi]$, $\E[\theta]$, and $\E[\epsilon]$, as shown in Eq.~\ref{eq:omega} \;
			\textbf{set} $(K+T)$-vector $\E[z_{d,v}] = w_{d,v} * \omega_{d,v}$ \;
			\textbf{update} $\lambda^{\theta,shape}_d \pluseq \E[z^\mathcal{K}_{d,v}]$ (Eq.~\ref{eq:theta})\;
			\textbf{update} $\lambda^{\epsilon,shape}_d \pluseq \E[z^\mathcal{K}_{d,v}]$ (Eq.~\ref{eq:epsilon})\;
			\textbf{update} $\lambda^{\beta}_v \pluseq \E[z^\mathcal{K}_{d,v}]$ (Eq.~\ref{eq:beta})\;
			\textbf{update} $\lambda^{\pi}_v \pluseq \E[z^\mathcal{T}_{d,v}]$ (Eq.~\ref{eq:pi})\;
		}
		\textbf{update} $\lambda^{\theta,rate}_{d} \pluseq \E[\phi_{a_d}]$ (Eq.~\ref{eq:theta})\;
		\textbf{update} $\lambda^{\epsilon,rate}_{d} \pluseq \E[\psi]$ (Eq.~\ref{eq:epsilon})\;
		\BlankLine
		\textbf{set} $\E[\theta_d] = \lambda^{\theta,shape}_d / \lambda^{\theta,rate}_d$ \;
		\textbf{set} $\E[\epsilon_d] = \lambda^{\epsilon,shape}_d / \lambda^{\epsilon,rate}_d$ \;
		\BlankLine
		\textbf{update} $\lambda^{\phi,shape}_{a_d} \pluseq s_\theta$ (Eq.~\ref{eq:phi}) \;
		\textbf{update} $\lambda^{\psi,shape}_{t} \pluseq s_\epsilon \forall t : f(i_d, t) \neq 0$ (Eq.~\ref{eq:psi}) \;
		\textbf{update} $\lambda^{\phi,rate}_{a_d} \pluseq \theta_d$ (Eq.~\ref{eq:phi}) \;
		\textbf{update} $\lambda^{\psi,rate} \pluseq \epsilon_d$ (Eq.~\ref{eq:psi}) \;

	}

	\textbf{set} $\E[\phi] = \lambda^{\phi,shape} / \lambda^{\phi,rate}$ \;
	\textbf{set} $\E[\beta_k] = \lambda^{\beta_{k,v}} / \sum_v \lambda^{\beta_k} \forall k$ \;
	\textbf{set} $\E[\psi] = \lambda^{\psi,shape} / \lambda^{\psi,rate}$ \;
	\textbf{set} $\E[\pi_t] = \lambda^{\pi_{t,v}} / \sum_v \lambda^{\pi_t} \forall t$ \;
}
\Return{$\lambda$} \;
\caption{Variational Inference for Capsule}
\label{alg:capsule}
\end{algorithm}
\footnotetext{$V(d)$ is the set of vocabulary indices for the collection of words in document $d$.  We could also iterate over all $V$, but as zero word counts give $\E[z_{d,v}] = 0~\forall v \not\in V(d)$, the two are equivalent.}



