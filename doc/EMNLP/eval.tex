%!TEX root = emnlp2016.tex


% *** results ***
% (1) 1970s cables data; details
%     how we trimmed; how we selected the vocabulary
% (2) we fit the model; 100 topics
%     what we found
% (3) simulations
%     (a) how we simulated data; size, etc.
%     (b) competing methods
%     (c) results

% - divde into pghs
% - made f clear on our model
% - make comparison methods clear (citations)
% - make evaluation metrics clear (citation?)
% - how long does it take to fit the model
% - address simulate then fit
% - details on simuated data; do other methods scale?

% ========================

\PP In this section we first explore the performance of Capsule on a collection of U.S. State Department diplomatics cables, then we ??? on simulated data.

\parhead{Data.}
The National Archive collects communications between the U.S. Sate department and its embassies.  We obtained a collection of these diplomatic messages from the History Lab at Columbia,\footnote{http://history-lab.org} which received them from the Central Foreign Policy Files at the National Archives.  The communications in this data set were sent between 1973 and 1978.

In addition to the text of the cables themselves, each document is supplemented with information about who sent the cable (e.g., the State Department, the U.S. Embassy in Saigon, or an individual by name), who received the cable (often multiple entities), and the date the cable was sent.  We used a vocabulary of size 6,293 and omitted cables with fewer than three terms, resulting in a collection of 2,139,324 messages sent between 27,134 entities.  We selected a weekly duration for the time intervals, as few cables were sent on the weekends.

\parhead{Model settings.}
We fit Capsule with $K=100$ general topics and using an exponential decay $f$,
\begin{equation}
f(i_d, t) = 
\begin{cases}
    0,			& \text{if } t > i_d\\
    \exp\{-(i_d - t) / \tau\},          & \text{otherwise,}
\end{cases}
\end{equation}
with mean lifetime $\tau=3$.  This mean lifetime indicates that most intervals would no longer be relevant after about three weeks.

\parhead{Results.}
We begin our exploration by detecting events using Capsule.  With Equation~\ref{eq:eventness} as our metric of ``eventness,'' we consider this metric over time, which is shown in Figure~\ref{fig:cables_events}.  Here, peaks correspond to real-worlds events, several of which are labeled.\footnote{Appendix~\ref{sec:additional_results} contains an analogous figure on arXiv data, which shows that Capsule does not capture weekly events on data that does not contain real-world events at that resolution.}

The tallest peak occurs the week of December 1, 1975, just prior to the Indonesian invasion of East Timor, which began December 7, 1975.  As discussed in Section~\ref{sec:model}, we sort documents by their event relevancy parameters $\epsilon$ to find cables that reflect an event.  Table~\ref{tab:timor} shows the top cables for the East Timor invasion.  Capsule accurately identifies this real-world event and recovers relevant cables.

\begin{table*}[tb]
\small
\centering
\begin{tabular}{cccl}
\toprule
$\epsilon$ & date & entity & subject \\
\midrule
0.124   &  1975-12-03  &  State  & President's talking point on Portuguese Timor \\
0.121   &  1975-12-03  &  State  & President's talking point on Portuguese Timor \\
0.115   &  1975-12-04  &  State  & Timor we are repeating FYI a DAO message \\
0.112   &  1975-12-04  &  State  &  Legal problems relating to Portuguese Timor\\
0.105   &  1975-12-04  &  Secretary Peking & US Support for Timor resolution \\
0.102   &  1975-12-07  &  State  & Invasion of Portuguese Timor \\
\bottomrule
\end{tabular}
\label{tab:timor}
\caption{Top documents for the time interval of week December 1, 1975, just prior to the Indonesian invasion of East Timor, which began December 7, 1975.}
\end{table*}

The second tallest peak occurs the week of April 21, 1975, just prior to the fall of Saigon on April 30, 1975; Table~\ref{tab:saigon} shows the top cables for this event, which reflect the evacuation efforts that occurred during that week.  Unlike the East Timor event, where the most relevant communication exists at an administrative level, the evacuation of Saigon is best captured by individuals seeking help for family and friends.

\begin{table*}[htb]
\small
\centering
\begin{tabular}{cccl}
\toprule
$\epsilon$ & date & entity & subject \\
\midrule
0.090   &  1975-04-24  &  Mansfield, Mike & Assistance in evacuating family from South Vietnam \\
0.089   &  1975-04-24  &  Railsback, Tom & Assistance in evacuating friend from South Vietnam \\
0.088   &  1975-04-24  &  Mansfield, Mike  & Assistance in evacuating family from South Vietnam \\
0.086   &  1975-04-24  &  Williams, Harrison &  Assistance in evacuating family from South Vietnam \\
0.086   &  1975-04-24  &  Koch, Edward & Assistance in evacuating family from South Vietnam \\
0.086   &  1975-04-21  &  Schweiker, Richard & Support in evacuating family from Vietnam \\
$\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ \\
0.081   &  1975-04-25  &  Ketchum, William & Movement of South Vietnamese refugees to Guam \\
0.080   &  1975-04-21  &  Scott, Hugh & Whereabouts of missionaries in Vietnam \\
\bottomrule
\end{tabular}
\label{tab:saigon}
\caption{Top documents for the time interval of week April 21, 1975, just prior to the fall of Saigon on April 30, 1975.}
\end{table*}

Smaller events...

%WORKING LINE

\PP highlight a few events, recover cables for them (1 pgh per event).  Point to appendix for more cable contnet and more events

\PP maybe: top terms for a selection of events (needs new table)

\PP top terms for a selectoion of topics (point to appendix for full list?)

\PP top terms for a selection of entities (point to appendix for full list)

\PP saying that this method is awesome!

\parhead{Simulations.}
\PP how did we simulate data; note on why this is okay

\PP evaluation metrics; w/ citations

\PP comparison methods; w/ citations

\PP eventness results

\PP document recovery results

\PP note on sensitivity to duration/shape and more simulations in Appendix~\ref{sec:additional_results}


Figure~\ref{fig:cables_events} shows this measure over the duration of the data set. The highest time intervals, ones in which we declare events to be detected, include the tallest peak the week of December 1, 1975, just prior to the Indonesian invasion of East Timor, which Began December 7, 1975.  The second tallest peak occurs the week of April 21, 1975, just prior to the fall of Saigon on April 30, 1975.  For any given week, we can sort the documents by their interval relevancy parameters $\epsilon$; Tables~\ref{tab:timor} and~\ref{tab:saigon} show the top cables for these two events, which reflect the real-world events those weeks.




Other event peaks include the week of July 2, 1973; the top three words under event its description $\pi$ are \emph{bicentennial}, \emph{hijack}, and \emph{mercenary}.  Top cables under event relevancy $\epsilon$ surround the bicentennial celebration of United States (July 4, 1973) and the Air France hijacking incident that began on June 27: Israeli operatives rescued hostages from this incident on July 4th.

Another peak occurs the week of April 17, 1978 surrounding a UN special session on disarmament; the top three words under event its description $\pi$ are \emph{SSOD} (acronym for ``special session on disarmament'', \emph{disarmament}, and \emph{ICS} (likely an acronym for ``incident command system'').

\begin{table*}
\centering
\small
\begin{tabular}{c}
\toprule
top terms \\
\midrule
OUTLOOK, REVIEW, HIRE, PERSONNEL, INVITE, PREPARE, NECESSARY \\
ARREST, INCIDENT, SECURITY, FAMILY, OFF, GUARD, DEATH, JAIL \\
LOCATE, HOME, SON, DEATH, PLEASE, CONTACT, FATHER, DEPARTMENT \\
REQUEST, REFUGEE, RESPONSE, SERVICE, SALE, ASYLUM, APPRECIATE \\
MARKET, REPORT, COPY, COMMERCIAL, FOOD, IMPORT, COMMERCE \\
FEAR, LEADERSHIP, BACK, ARM, ROLE, PLAY, THREATEN \\
HOTEL, TRAVEL, RESERVATION, VISIT, ARRANGE, SCHEDULE, STAY \\
\bottomrule
\end{tabular}
\label{tab:topics}
\caption{Top vocabulary terms for a selection of topics, according to topic distributions $\beta_k$.}
\end{table*}

\begin{table*}
\centering
\small
\begin{tabular}{cc}
\toprule
entity & top terms \\
\midrule
STATE & REQUEST, FOLLOW, EMBASSY, MEET, MAKE, STATE, DEPARTMENT \\
BANGKOK & BANGKOK, THAILAND, THAI, REFUGEE, EMBASSY, FOLLOW, REPORT \\
JERUSALEM & JERUSALEM, ISRAELI, BANK, REPORT, SAY, COMMENT, ONE \\
STOCKHOLM & SWEDISH, SWEDEN, TRADE, MEET, EMBASSY, FOLLOW, MAKE \\
CASABLANCA & CASABLANCA, MOROCCO, MOROCCAN, REQUEST, PLEASE, FOLLOW, NOTE \\
KAMPALA & UGANDAN, NAIROBI, AFRICAN, IMPERIALIST, VOICE, KENYA, MISSIONARY \\
NDJAMEAN & CHADIAN, CHAD, LAGOS, DROUGHT, INITIATION, AUSTERITY, GOC \\
\bottomrule
\end{tabular}
\label{tab:entities}
\caption{Top vocabulary terms for a selection of entities according to entity-exclusive topics $\beta^{(n)}_0$.}
\end{table*}

Examples of general topics of conversation are shown in Table~\ref{tab:topics} and entity-exclusive topics are shown in Table~\ref{tab:entities}; these show us how entity topics absorb location-specific words, preventing these terms from overwhelming the general topics.  

These exploratory results show that our model is successfully capturing when multiple entities are discussing the same subjects and that our model can be used to explore the underlying data by providing a structured scaffold from which to view the data.


\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{fig/sim_eventdetect.pdf}
\caption{Event detection performance on twenty simulated datasets.  Capsule is able to detect events as well as comparison methods, but its performance has higher variance.}
\label{fig:sim_eventdetect}
\end{figure}

Finally, we simulated data according to our generative process in order to compare our method to baseline and existing approaches.  
To evaluate event detection, we created a ranked list of all time intervals and computed the overlap between a model and the simulated ground at every threshold; this generates an curve under which we cam compute the area and normalized based on ideal performance---we refer to this metric as event detection AUC.  
The most successful of the comparison methods for event detection was average absolute error in wordcount, both unweighted and weighted by tf-idf.  Figure~\ref{fig:sim_eventdetect} shows that Capsule can outperform these approaches for event detection, but that it has higher variance in performance.  The other comparison method in Figure~\ref{fig:sim_eventdetect} is based on LDA; we fit a multinomial Gaussian to the topic representation of all documents and then computed the average probability of seeing the topic distributions of documents in the time interval.  Time intervals with the lowest probability were marked as most likely to have events.  All other baselines performed close to random for event detection.
\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{fig/precision10.pdf}
\caption{Precision of recovering the top ten most relevant documents, averaged over all time intervals.  Capsule performs best, averaged over twenty simulations.}
\label{fig:sim_precision}
\end{figure}

This method of fitting a multinomial Gaussian to LDA representations of documents also performed well for recovering relevant documents.  This approach can be altered to fit a per-entity multinomial Gaussian, but this performs worse.  Simply finding documents based on absolute deviation from the mean works well in LDA topic space (relative to overall mean or entity mean), but not over the full vocabulary.  Word count deviations, which performed well for event detection, performed worse than random for document recovery.  Both Capsule and its event-only partial model outperform all comparison methods in terms of document recovery.  Figure~\ref{fig:sim_precision} shows precision of recovering the top ten documents.

We assessed the sensitivity of our model to three different decay functions $f$: exponential, linear, and step functions.  We simulated data for each function and then fit Capsule using every permutation of $f$ and multiple settings for event decay duration.  In all cases, we found that the model is not sensitive to decay shape or duration.

% \begin{figure*}
% \centering
% \includegraphics[width=\linewidth]{fig/arxiv_events.pdf}
% \caption{TODO}
% \label{fig:arxiv_events}
% \end{figure*}






% In this section we study the performance of Capsule. Using simulated data, we compare Capsule to deterministic methods of event detection and show that Capsule outperforms them at identifying when events occur.  
% %We also examine how sensitive Capsule is to the attributes of a dataset and model parameters.
% We conclude by exploring three real-world datasets with Capsule.

% \subsection{Performance}

% We generated ten simulated datasets using our generative process.  Each dataset spans 100 days and contains content associated with ten entities.  Approximately ten events also exist in each dataset, randomly distributed in time and with a three day decay of relevancy.

% To evaluate performance, we rank each day by its probability of having an event occur, and plot the number of true events discovered against the number of false positive events, as shown in Figure~\ref{fig:sim_auc}; the area under the curve (AUC) can be computed for a single evaluation metric.  Note that this approach is only valid when true events are known, and thus we only apply it to simulated data.

% We compare Capsule to two baseline approaches: one considers the greatest document outlier on a given day--days with the furthest outliers are the most likely to have events.  The other approach is similar: days are represented by an average of all documents associated with that day, and one considers how these averages deviate from the global average--the further away, the more likely an event.

% Figure~\ref{fig:sim_auc} shows that Capsule outperforms both of these approaches.  It should be noted that inference on Capsule will produce different results, depending on the random seed; the results shown are the best of three random seeds.

% \begin{figure}[ht]
% \centering
% \includegraphics[width=\linewidth]{fig/sim_auc.pdf}
% \caption{Average performance on ten simulated datasets; lines closer to the upper-left are better.  Baselines consider outliers based on full corpus averages (dashed) and averages of all entity documents (dotted).  Capsule performance is best of three random seeds.}
% \label{fig:sim_auc}
% \end{figure}


% %\PP fig / discussion of varrying entity dists in simualted data

% %\PP performance vs parameters (event occurrence hyperparameter (which impacts initialization), event duration)

% \subsection{Exploration}

% \parhead{Cables}
% \PP where did we get it / size / preprocessing

% \PP plot of events timeline with select real-world match evetns pointed out (verified by history lab)

% \PP example interesting entities + figure

% \PP explore pairwise entities? (quick with and single figure shared with enron); compare sender vs reiever for same pair  (or does direction matter?? tyr both ways) look at sender in norma model vs sender in a few pairs under this construction 

% \parhead{arXiv}
% \PP where did we get it / size / preprocessing

% \PP plot of events timeline with select real-world match evetns pointed out (verified by history lab)

% \PP example interesting entities + figure

% \parhead{enron}
% \PP where did we get it / size / preprocessing

% \PP plot of events timeline with select real-world match evetns pointed out (verified by history lab)

% \PP example interesting entities + figure

% \PP explore pairwise entities?



% % \parhead{Cables}
% % We obtained around two million of these cables sent between 1973 and 1977 via the History Lab at Columbia,\footnote{http://history-lab.org} which received them from the Central Foreign Policy Files at the National Archives.  In addition to the text of the cables themselves, each document is supplemented with information about who sent the cable (e.g., the State Department, the U.S. Embassy in Saigon, or an individual by name), who received the cable (often multiple entities), and the date the cable was sent.
% % Excerpts from three example cables are shown in Figure~\ref{fig:cables_example}.

% % % \begin{figure}[ht]
% % % \centering
% % % \includegraphics[width=\textwidth]{../fig/cables_orphan_example.png}
% % % \caption{Example excerpts of cables sent in April 1975 concerning orphans from the Vietnam War.}
% % % \label{fig:cables_example}
% % % \end{figure}

% % \parhead{arXiv}

% % \parhead{Enron}


% % \PP insert table and refetence for both (number of days, entities, total messages, or something); maybe a plot showing attributes of the data...somehow inform them that the state department is a bias for the cables data

% % \PP footnote on handling multiple recipients of message...

% % \subsection{Metrics and competing methods}

% % \PP how we evaluate based on real events

% % \PP how we evaluate based on perplexity (prediction of words)

% % \PP competing methods for perplexity: LDA, average user words?, dynamic topic model, network topic models

% % \subsection{Performance and exploration}

% % \PP sumry of comparison to gold-standard events for cables

% % \PP table of predictive likelihood results and summary pgh; and/or cite tea leaves paper

% % \parhead{Exploration}

% % \PP charachetrize events manually (based on cables) vs event detection characterization

% % \PP show descriptions for cables entities and select events; same for arxiv/enron

% % \PP any other exploration you can think of!


% % Results: ROC curve (x=false postive rate, y=true postive rate)