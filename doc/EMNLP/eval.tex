%!TEX root = emnlp2016.tex

In this section we explore the performance of Capsule on a collection of US State Department cables.
These cables were sent between 1973 and 1978 and obtained from the History Lab at Columbia,\footnote{http://history-lab.org} which received them from the Central Foreign Policy Files at the National Archives. In addition to the text of the cables themselves, each document is supplemented with information about who sent the cable (e.g., the State Department, the U.S. Embassy in Saigon, or an individual by name), who received the cable (often multiple entities), and the date the cable was sent.  To test our model, we used a vocabulary of size 6,293 and omitted cables with fewer than three terms, resulting in a collection of 2,139,324 messages sent between 27,134 entities.  We selected a weekly duration for the time intervals, as few cables were sent on the weekends.

We fit Capsule with 100 topics and using an exponential decay with mean lifetime of 3---this indicates that most intervals would no longer be relevant after about 3 weeks.  To detect when an event occurs, we multiply the average event relevancy $\epsilon$ for all documents in a given interval together with interval strength $\psi$, or  $\psi_t \frac{1}{\vert D_t \vert}\sum_{d\in D_t} \epsilon_{d,t}$, where $D_t$ is the set of all cables sent in interval $t$.

Figure~\ref{fig:cables_events} shows this measure over the duration of the data set. The highest time intervals, ones in which we declare events to be detected, include the tallest peak the week of December 1, 1975, just prior to the Indonesian invasion of East Timor, which Began December 7, 1975.  The second tallest peak occurs the week of April 21, 1975, just prior to the fall of Saigon on April 30, 1975.  For any given week, we can sort the documents by their interval relevancy parameters $\epsilon$; Tables~\ref{tab:timor} and~\ref{tab:saigon} show the top cables for these two events, which reflect the real-world events those weeks.

\begin{table*}[tb]
\small
\centering
\begin{tabular}{cccl}
\toprule
$\epsilon$ & date & entity & subject \\
\midrule
0.1237 & 1975-12-03 & {\footnotesize STATE} & {\footnotesize PRESIDENT'S TALKING POINTS ON PORTUGUESE TIMOR } \\
0.1210   &  1975-12-03 & {\footnotesize STATE} & {\footnotesize    PRESIDENT'S TALKING POINTS ON PORTUGUESE TIMOR }\\
0.1153   &  1975-12-04  &  {\footnotesize STATE  }  & {\footnotesize  TIMOR WE ARE REPEATING FYI A DAO MESSAGE}\\
0.1126   &  1975-012-04  &  {\footnotesize STATE   }  &  {\footnotesize  LEGAL PROBLEMS RELATING TO PORTUGUESE TIMOR}\\
0.1053   &  1975-12-07  &  {\footnotesize SECRETARY PEKING } & {\footnotesize US SUPPORT FOR TIMOR RESOLUTION }\\
0.1021   &  1975-12-01  &  {\footnotesize STATE } & {\footnotesize  INVASION OF PORTUGUESE TIMOR } \\
\bottomrule
\end{tabular}
\label{tab:timor}
\caption{Top documents for the time interval of week December 1, 1975, just prior to the Indonesian invasion of East Timor, which Began December 7, 1975.}
\end{table*}


\begin{table*}[htb]
\small
\centering
\begin{tabular}{cccl}
\toprule
$\epsilon$ & date & entity & subject \\
\midrule
0.0908 & 1975-04-24 & {\footnotesize MANSFIELD, MIKE} & {\footnotesize ASSISTANCE IN EVACUATING FAMILY FROM SOUTH VIETNAM } \\
0.0886   &  1975-04-24 & {\footnotesize RAILSBACK, TOM } & {\footnotesize    ASSISTANCE IN EVACUATING FRIEND FROM SOUTH VIETNAM }\\
0.0877   &  1975-04-24  &  {\footnotesize MANSFIELD, MIKE  }  & {\footnotesize    ASSISTANCE IN EVACUATING FAMILY FROM SOUTH VIETNAM}\\
0.0863   &  1975-04-24  &  {\footnotesize WILLIAMS, HARRISON   }  &  {\footnotesize    ASSISTANCE IN EVACUATING FAMILY FROM VIETNAM}\\
0.0860   &  1975-04-24  &  {\footnotesize KOCH, EDWARD } & {\footnotesize ASSISTANCE IN EVACUATING FAMILY FROM SOUTH VIETNAM }\\
0.0858   &  1975-04-21  &  {\footnotesize SCHWEIKER, RICHARD } & {\footnotesize  SUPPORT IN EVACUATING FAMILY FROM VIETNAM } \\
$\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ \\
0.0812  &   1975-04-25  &  {\footnotesize KETCHUM, WILLIAM} & {\footnotesize  MOVEMENT OF SOUTH VIETNAMESE REFUGEES TO GUAM } \\
0.0800 & 1975-04-21 & {\footnotesize SCOTT, HUGH } & {\footnotesize  WHEREABOUTS OF MISSIONARIES IN VIETNAM }\\
\bottomrule
\end{tabular}
\label{tab:saigon}
\caption{Top documents for the time interval of week April 21, 1975, just prior to the fall of Saigon on April 30, 1975.}
\end{table*}

Other event peaks include the week of July 2, 1973; the top three words under event its description $\pi$ are \emph{bicentennial}, \emph{hijack}, and \emph{mercenary}.  Top cables under event relevancy $\epsilon$ surround the bicentennial celebration of United States (July 4, 1973) and the Air France hijacking incident that began on June 27: Israeli operatives rescued hostages from this incident on July 4th.

Another peak occurs the week of April 17, 1978 surrounding a UN special session on disarmament; the top three words under event its description $\pi$ are \emph{SSOD} (acronym for ``special session on disarmament'', \emph{disarmament}, and \emph{ICS} (likely an acronym for ``incident command system'').

\begin{table*}
\centering
\small
\begin{tabular}{c}
\toprule
top terms \\
\midrule
OUTLOOK, REVIEW, HIRE, PERSONNEL, INVITE, PREPARE, NECESSARY \\
ARREST, INCIDENT, SECURITY, FAMILY, OFF, GUARD, DEATH, JAIL \\
LOCATE, HOME, SON, DEATH, PLEASE, CONTACT, FATHER, DEPARTMENT \\
REQUEST, REFUGEE, RESPONSE, SERVICE, SALE, ASYLUM, APPRECIATE \\
MARKET, REPORT, COPY, COMMERCIAL, FOOD, IMPORT, COMMERCE \\
FEAR, LEADERSHIP, BACK, ARM, ROLE, PLAY, THREATEN \\
HOTEL, TRAVEL, RESERVATION, VISIT, ARRANGE, SCHEDULE, STAY \\
\bottomrule
\end{tabular}
\label{tab:topics}
\caption{Top vocabulary terms for a selection of topics, according to topic distributions $\beta_k$.}
\end{table*}

\begin{table*}
\centering
\small
\begin{tabular}{cc}
\toprule
entity & top terms \\
\midrule
STATE & REQUEST, FOLLOW, EMBASSY, MEET, MAKE, STATE, DEPARTMENT \\
BANGKOK & BANGKOK, THAILAND, THAI, REFUGEE, EMBASSY, FOLLOW, REPORT \\
JERUSALEM & JERUSALEM, ISRAELI, BANK, REPORT, SAY, COMMENT, ONE \\
STOCKHOLM & SWEDISH, SWEDEN, TRADE, MEET, EMBASSY, FOLLOW, MAKE \\
CASABLANCA & CASABLANCA, MOROCCO, MOROCCAN, REQUEST, PLEASE, FOLLOW, NOTE \\
KAMPALA & UGANDAN, NAIROBI, AFRICAN, IMPERIALIST, VOICE, KENYA, MISSIONARY \\
NDJAMEAN & CHADIAN, CHAD, LAGOS, DROUGHT, INITIATION, AUSTERITY, GOC \\
\bottomrule
\end{tabular}
\label{tab:entities}
\caption{Top vocabulary terms for a selection of entities according to entity-exclusive topics $\beta^{(n)}_0$.}
\end{table*}

Examples of general topics of conversation are shown in Table~\ref{tab:topics} and entity-exclusive topics are shown in Table~\ref{tab:entities}; these show us how entity topics absorb location-specific words, preventing these terms from overwhelming the general topics.  

These exploratory results show that our model is successfully capturing when multiple entities are discussing the same subjects and that our model can be used to explore the underlying data by providing a structured scaffold from which to view the data.


\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{fig/sim_eventdetect.pdf}
\caption{Event detection performance on twenty simulated datasets.  Capsule is able to detect events as well as comparison methods, but its performance has higher variance.}
\label{fig:sim_eventdetect}
\end{figure}

Finally, we simulated data according to our generative process in order to compare our method to baseline and existing approaches.  
To evaluate event detection, we created a ranked list of all time intervals and computed the overlap between a model and the simulated ground at every threshold; this generates an curve under which we cam compute the area and normalized based on ideal performance---we refer to this metric as event detection AUC.  
The most successful of the comparison methods for event detection was average absolute error in wordcount, both unweighted and weighted by tf-idf.  Figure~\ref{fig:sim_eventdetect} shows that Capsule can outperform these approaches for event detection, but that it has higher variance in performance.  The other comparison method in Figure~\ref{fig:sim_eventdetect} is based on LDA; we fit a multinomial Gaussian to the topic representation of all documents and then computed the average probability of seeing the topic distributions of documents in the time interval.  Time intervals with the lowest probability were marked as most likely to have events.  All other baselines performed close to random for event detection.
\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{fig/precision10.pdf}
\caption{Precision of recovering the top ten most relevant documents, averaged over all time intervals.  Capsule performs best, averaged over twenty simulations.}
\label{fig:sim_precision}
\end{figure}

This method of fitting a multinomial Gaussian to LDA representations of documents also performed well for recovering relevant documents.  This approach can be altered to fit a per-entity multinomial Gaussian, but this performs worse.  Simply finding documents based on absolute deviation from the mean works well in LDA topic space (relative to overall mean or entity mean), but not over the full vocabulary.  Word count deviations, which performed well for event detection, performed worse than random for document recovery.  Both Capsule and its event-only partial model outperform all comparison methods in terms of document recovery.  Figure~\ref{fig:sim_precision} shows precision of recovering the top ten documents.

We assessed the sensitivity of our model to three different decay functions $f$: exponential, linear, and step functions.  We simulated data for each function and then fit Capsule using every permutation of $f$ and multiple settings for event decay duration.  In all cases, we found that the model is not sensitive to decay shape or duration.

% \begin{figure*}
% \centering
% \includegraphics[width=\linewidth]{fig/arxiv_events.pdf}
% \caption{TODO}
% \label{fig:arxiv_events}
% \end{figure*}






% In this section we study the performance of Capsule. Using simulated data, we compare Capsule to deterministic methods of event detection and show that Capsule outperforms them at identifying when events occur.  
% %We also examine how sensitive Capsule is to the attributes of a dataset and model parameters.
% We conclude by exploring three real-world datasets with Capsule.

% \subsection{Performance}

% We generated ten simulated datasets using our generative process.  Each dataset spans 100 days and contains content associated with ten entities.  Approximately ten events also exist in each dataset, randomly distributed in time and with a three day decay of relevancy.

% To evaluate performance, we rank each day by its probability of having an event occur, and plot the number of true events discovered against the number of false positive events, as shown in Figure~\ref{fig:sim_auc}; the area under the curve (AUC) can be computed for a single evaluation metric.  Note that this approach is only valid when true events are known, and thus we only apply it to simulated data.

% We compare Capsule to two baseline approaches: one considers the greatest document outlier on a given day--days with the furthest outliers are the most likely to have events.  The other approach is similar: days are represented by an average of all documents associated with that day, and one considers how these averages deviate from the global average--the further away, the more likely an event.

% Figure~\ref{fig:sim_auc} shows that Capsule outperforms both of these approaches.  It should be noted that inference on Capsule will produce different results, depending on the random seed; the results shown are the best of three random seeds.

% \begin{figure}[ht]
% \centering
% \includegraphics[width=\linewidth]{fig/sim_auc.pdf}
% \caption{Average performance on ten simulated datasets; lines closer to the upper-left are better.  Baselines consider outliers based on full corpus averages (dashed) and averages of all entity documents (dotted).  Capsule performance is best of three random seeds.}
% \label{fig:sim_auc}
% \end{figure}


% %\PP fig / discussion of varrying entity dists in simualted data

% %\PP performance vs parameters (event occurrence hyperparameter (which impacts initialization), event duration)

% \subsection{Exploration}

% \parhead{Cables}
% \PP where did we get it / size / preprocessing

% \PP plot of events timeline with select real-world match evetns pointed out (verified by history lab)

% \PP example interesting entities + figure

% \PP explore pairwise entities? (quick with and single figure shared with enron); compare sender vs reiever for same pair  (or does direction matter?? tyr both ways) look at sender in norma model vs sender in a few pairs under this construction 

% \parhead{arXiv}
% \PP where did we get it / size / preprocessing

% \PP plot of events timeline with select real-world match evetns pointed out (verified by history lab)

% \PP example interesting entities + figure

% \parhead{enron}
% \PP where did we get it / size / preprocessing

% \PP plot of events timeline with select real-world match evetns pointed out (verified by history lab)

% \PP example interesting entities + figure

% \PP explore pairwise entities?



% % \parhead{Cables}
% % We obtained around two million of these cables sent between 1973 and 1977 via the History Lab at Columbia,\footnote{http://history-lab.org} which received them from the Central Foreign Policy Files at the National Archives.  In addition to the text of the cables themselves, each document is supplemented with information about who sent the cable (e.g., the State Department, the U.S. Embassy in Saigon, or an individual by name), who received the cable (often multiple entities), and the date the cable was sent.
% % Excerpts from three example cables are shown in Figure~\ref{fig:cables_example}.

% % % \begin{figure}[ht]
% % % \centering
% % % \includegraphics[width=\textwidth]{../fig/cables_orphan_example.png}
% % % \caption{Example excerpts of cables sent in April 1975 concerning orphans from the Vietnam War.}
% % % \label{fig:cables_example}
% % % \end{figure}

% % \parhead{arXiv}

% % \parhead{Enron}


% % \PP insert table and refetence for both (number of days, entities, total messages, or something); maybe a plot showing attributes of the data...somehow inform them that the state department is a bias for the cables data

% % \PP footnote on handling multiple recipients of message...

% % \subsection{Metrics and competing methods}

% % \PP how we evaluate based on real events

% % \PP how we evaluate based on perplexity (prediction of words)

% % \PP competing methods for perplexity: LDA, average user words?, dynamic topic model, network topic models

% % \subsection{Performance and exploration}

% % \PP sumry of comparison to gold-standard events for cables

% % \PP table of predictive likelihood results and summary pgh; and/or cite tea leaves paper

% % \parhead{Exploration}

% % \PP charachetrize events manually (based on cables) vs event detection characterization

% % \PP show descriptions for cables entities and select events; same for arxiv/enron

% % \PP any other exploration you can think of!


% % Results: ROC curve (x=false postive rate, y=true postive rate)