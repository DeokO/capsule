%
% File emnlp2016.tex
%

\documentclass[11pt,letterpaper]{article}

\usepackage{emnlp2016}
\usepackage{times}
\usepackage{latexsym}

\input{preamble/preamble.tex}
\input{preamble/preamble_math.tex}
\input{preamble/preamble_acronyms.tex}


% Uncomment this line for the final submission:
\emnlpfinalcopy

%  Enter the EMNLP Paper ID here:
\def\emnlppaperid{***} % TODO

% To expand the titlebox for more authors, uncomment
% below and set accordingly.
% \addtolength\titlebox{.5in}    

\newcommand\BibTeX{B{\sc ib}\TeX}


\title{Relationship between LDA and PF \\ (for use in thesis)}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}
% If the title and author information does not fit in the area allocated,
% place \setlength\titlebox{<new height>} right after
% at the top, where <new height> can be something larger than 2.25in
\author{
Allison J.B. Chaney\\
    Princeton University\\
	\href{mailto:achaney@cs.princeton.edu}{\nolinkurl{achaney@cs.princeton.edu}}
}

\date{}

\begin{document}

\maketitle


\section{Introduction}
TODO

\section{Background}

\begin{figure}
\begin{mdframed}
\small
\begin{itemize}[leftmargin=*]
\item for each topic $k=$~1:$K$,
	\begin{itemize}[leftmargin=*]
		\item draw topic distribution over vocabulary \\$\beta_k \sim \mbox{Dirichlet}_V (\alpha_\beta)$
	\end{itemize}
\item for each document $d=$~1:$D$,
	\begin{itemize}[leftmargin=*]
		\item draw local document topics \\$\theta_d \sim \mbox{Dirichlet}_K(\alpha_\theta)$
		\item for each word $n=$~1:$N_d$,
		\begin{itemize}[leftmargin=*]
			\item draw word assignment \\$z_{d,n} \sim \mbox{Categorical}\left(\theta_d\right)$
			\item draw word $w_{d,n} \sim \mbox{Categorical}\left(\beta_{z_{d,n}}\right)$
		\end{itemize}
	\end{itemize}
\end{itemize}
\end{mdframed}
\caption{The generative process for LDA.}
\label{fig:generative-LDA}
\end{figure}

\begin{figure}
\begin{mdframed}
\small
\begin{itemize}[leftmargin=*]
\item for each topic $k=$~1:$K$,
	\begin{itemize}[leftmargin=*]
		\item for each term topic $v=$~1:$V$,
		\begin{itemize}[leftmargin=*]
			\item draw topic distribution over vocabulary \\$\beta_{k,v} \sim \mbox{Gamma} (s_\beta, r_\beta)$
		\end{itemize}
	\end{itemize}
\item for each document $d=$~1:$D$,
	\begin{itemize}[leftmargin=*]
		\item for each topic $k=$~1:$K$,
		\begin{itemize}[leftmargin=*]
			\item draw local document topics \\$\theta_{d,k} \sim \mbox{Gamma}(s_\theta, v_\theta)$
		\end{itemize}
		\item for each vocabulary term $v=$~1:$V$,
		\begin{itemize}[leftmargin=*]
			\item draw word counts \\$w_{d,v} \sim \mbox{Poisson}\left(\theta_d^\top\beta_v\right)$
		\end{itemize}
	\end{itemize}
\end{itemize}
\end{mdframed}
\caption{The generative process for PF.}
\label{fig:generative-PF}
\end{figure}

- inference algorithm for LDA
- inference algorithm for PF



\begin{algorithm}
\small
\DontPrintSemicolon
\KwIn{words $w$}
\KwOut{approximate posterior of latent parameters (topics $\beta$, document topics $\theta$, and word assignments $z$) in terms of variational parameters $\lambda$}
\textbf{Initialize} $\E[\beta]$ to slightly random around uniform \;
\textbf{Initialize} $\E[\theta]$ to uniform \;
\For {iteration $i=1:M$}{
	\textbf{set} $\lambda^\beta$ and $\lambda^\theta$ to respective priors \;
	\textbf{update} $\lambda^{\theta,rate} += \sum_V \E[\beta_v]$ \;
	\For {each document $d=1:D$}{
		\For {each term $v\in V(d)$\footnotemark}{
			\textbf{set} $k$-vector $\phi_{d,v} \propto \langle \E[\theta_{d,1}] \E[\beta_{1,v}], \dots, \E[\theta_{d,K}] \E[\beta_{K,v}],\rangle$\;
			\textbf{set} $k$-vector $\E[z_{d,v}] = w_{d,v} * \phi_{d,v}$ \;
			\textbf{update} $\lambda^{\theta,shape}_d += \E[z_{d,v}]$ \;
			\textbf{update} $\lambda^{\beta,shape}_d += \E[z_{d,v}]$ \;
		}
		\textbf{set} $\E[\theta_d] = \lambda^{\theta,shape}_d / \lambda^{\theta,rate}_d$ \;
		\textbf{update} $\lambda^{\beta,rate} += \E[\theta_d]$ \;
	}
	\textbf{set} $\E[\beta_d] = \lambda^{\beta,shape}_d / \lambda^{\beta,rate}_d$ \;
}
\Return{$\lambda$} \;
\caption{Variational Inference for LDA}
\label{alg:LDA}
\end{algorithm}


\begin{algorithm}
\small
\DontPrintSemicolon
\KwIn{word counts $w$}
\KwOut{approximate posterior of latent parameters (topics $\beta$ and document topics $\theta$) in terms of variational parameters $\lambda$}
\textbf{Initialize} $\E[\beta]$ to slightly random around uniform \;
\textbf{Initialize} $\E[\theta]$ to uniform \;
\For {iteration $i=1:M$}{
	\textbf{set} $\lambda^\beta$ and $\lambda^\theta$ to respective priors \;
	\textbf{update} $\lambda^{\theta,rate} += \sum_V \E[\beta_v]$ \;
	\For {each document $d=1:D$}{
		\For {each term $v\in V(d)$\footnotemark}{
			\textbf{set} $k$-vector $\phi_{d,v} \propto \langle \E[\theta_{d,1}] \E[\beta_{1,v}], \dots, \E[\theta_{d,K}] \E[\beta_{K,v}],\rangle$\;
			\textbf{set} $k$-vector $\E[z_{d,v}] = w_{d,v} * \phi_{d,v}$ \;
			\textbf{update} $\lambda^{\theta,shape}_d += \E[z_{d,v}]$ \;
			\textbf{update} $\lambda^{\beta,shape}_d += \E[z_{d,v}]$ \;
		}
		\textbf{set} $\E[\theta_d] = \lambda^{\theta,shape}_d / \lambda^{\theta,rate}_d$ \;
		\textbf{update} $\lambda^{\beta,rate} += \E[\theta_d]$ \;
	}
	\textbf{set} $\E[\beta_d] = \lambda^{\beta,shape}_d / \lambda^{\beta,rate}_d$ \;
}
\Return{$\lambda$} \;
\caption{Variational Inference for PF}
\label{alg:PF}
\end{algorithm}
\footnotetext{$V(d)$ is the set of vocabulary indices for the collection of words in document $d$.  We could also iterate over all $V$, but as zero word counts give $\E[z_{d,v}] = 0~\forall v \not\in V(d)$, the two are equivalent.}

TDOO: 
Discussion of mathematical similarity?
- PF make some subtle assumptions about word counts (wrapped up in theta)

\section{Hybrid model}
\begin{figure}
\begin{mdframed}
\small
\begin{itemize}[leftmargin=*]
\item for each topic $k=$~1:$K$,
	\begin{itemize}[leftmargin=*]
		\item draw topic distribution over vocabulary \\$\beta_k \sim \mbox{Dirichlet}_V (\alpha)$
	\end{itemize}
\item for each document $d=$~1:$D$,
	\begin{itemize}[leftmargin=*]
		\item for each topic $k=$~1:$K$,
		\begin{itemize}[leftmargin=*]
			\item draw local document topics \\$\theta_{d,k} \sim \mbox{Gamma}(s_\theta, v_\theta)$
		\end{itemize}
		\item for each vocabulary term $v=$~1:$V$,
		\begin{itemize}[leftmargin=*]
			\item draw word counts \\$w_{d,v} \sim \mbox{Poisson}\left(\theta_d^\top\beta_v\right)$
		\end{itemize}
	\end{itemize}
\end{itemize}
\end{mdframed}
\caption{The generative process for hybrid.}
\label{fig:generative-hybrid}
\end{figure}

\begin{algorithm}
\small
\DontPrintSemicolon
\KwIn{word counts $w$}
\KwOut{approximate posterior of latent parameters (topics $\beta$ and document topics $\theta$) in terms of variational parameters $\lambda$}
\textbf{Initialize} $\E[\beta]$ to slightly random around uniform \;
\textbf{Initialize} $\E[\theta]$ to uniform \;
\For {iteration $i=1:M$}{
	\textbf{set} $\lambda^\beta$ and $\lambda^\theta$ to respective priors \;
	\textbf{update} $\lambda^{\theta,rate} += \sum_V \E[\beta_v]$ \;
	\For {each document $d=1:D$}{
		\For {each term $v\in V(d)$\footnotemark}{
			\textbf{set} $k$-vector $\phi_{d,v} \propto \langle \E[\theta_{d,1}] \E[\beta_{1,v}], \dots, \E[\theta_{d,K}] \E[\beta_{K,v}],\rangle$\;
			\textbf{set} $k$-vector $\E[z_{d,v}] = w_{d,v} * \phi_{d,v}$ \;
			\textbf{update} $\lambda^{\theta,shape}_d += \E[z_{d,v}]$ \;
			\textbf{update} $\lambda^{\beta,shape}_d += \E[z_{d,v}]$ \;
		}
		\textbf{set} $\E[\theta_d] = \lambda^{\theta,shape}_d / \lambda^{\theta,rate}_d$ \;
		\textbf{update} $\lambda^{\beta,rate} += \E[\theta_d]$ \;
	}
	\textbf{set} $\E[\beta_d] = \lambda^{\beta,shape}_d / \lambda^{\beta,rate}_d$ \;
}
\Return{$\lambda$} \;
\caption{Variational Inference for Hybrid (copied from PF, no changes yet)}
\label{alg:hybrid}
\end{algorithm}

\section{Empirical results}

\appendix
\newpage
\onecolumn
\section{Derivation of LDA Updates}
\begin{align}
p(\beta_k \g w, \theta, z) &\propto  p(w, \theta, \beta, z) \\
&= \prod_{d=1}^D \left[ p(\theta_d \g \alpha_\theta) \prod_{n=1}^{N_d} \left[ p(z_{d,n} \g \theta_d) p(w_{d,n} \g \beta, z_{d,n})\right] \right] \prod_{k=1}^K p(\beta_k \g \alpha_\beta) \\
\log p(\beta_k \g w, \theta, z) &\propto \sum_{d=1}^D \left[ \log p(\theta_d \g \alpha_\theta) + \sum_{n=1}^{N_d} \left[ \log p(z_{d,n} \g \theta_d) + \log p(w_{d,n} \g \beta, z_{d,n})\right] \right] + \sum_{k=1}^K \log p(\beta_k \g \alpha_\beta) \\
&\propto \sum_{d=1}^D \sum_{n=1}^{N_d} \log p(w_{d,n} \g \beta, z_{d,n}) + \log p(\beta_k \g \alpha_\beta) \\
&= \sum_{d=1}^D \sum_{n=1}^{N_d} \log \mbox{Cat}(w_{d,n} \g \beta_{z_{d,n}}) + \sum_{v=1}^V (\alpha_\beta-1) \log \beta_{k,v} - \log \mathbf{B}(\bar{\alpha}_\beta) \\
&\propto \sum_{d=1}^D \sum_{n=1}^{N_d} \mathbf{1}[z_{d,n} = k] \log \mbox{Cat}(w_{d,n} \g \beta_k) + \sum_{v=1}^V (\alpha_\beta-1) \log \beta_{k,v} \\
&= \sum_{d=1}^D \sum_{n=1}^{N_d} \mathbf{1}[z_{d,n} = k] \sum_{v=1}^V w_{d,n,v} \log \beta_{k,v} + \sum_{v=1}^V (\alpha_\beta -1) \log \beta_{k,v} \\
&= \sum_{v=1}^K \log \beta_{k,v} \left[ \alpha_\beta - 1 + \sum_{d=1}^D \sum_{n=1}^{N_d} \mathbf{1}[z_{d,n} = k] w_{d,n,v} \right] \\
&\propto \mbox{Dirichlet}_V\left(\alpha_\beta + \sum_{d=1}^D \sum_{n=1}^{N_d} \mathbf{1}[z_{d,n} = k] w_{d,n,v}\right) \\
\beta_{k} \g \cdots &\sim \mbox{Dirichlet}_V\left(\alpha_\beta + \sum_{d=1}^D \sum_{n=1}^{N_d} \mathbf{1}[z_{d,n} = k] w_{d,n,v}\right) \\
\beta'_{k,v} \g \cdots &\sim \mbox{Gamma}\left(\alpha_\beta + \sum_{d=1}^D \sum_{n=1}^{N_d} \mathbf{1}[z_{d,n} = k] w_{d,n,v}, 1\right)
\end{align}


\section{Derivation of PF Updates}
\begin{align}
p(\beta_{k,v} \g w, \theta) &\propto  p(w, \theta, \beta) \\
&= \prod_{d=1}^D\prod_{v=1}^V p(w_{d,v} \g \theta_d, \beta_v)  \prod_{k=1}^K \left[ \prod_{v=1}^V p(\beta_{k,v} \g s_\beta, r_\beta) \prod_{d=1}^D p(\theta_{d,k} \g s_\beta, r_\beta)\right] \\
&= \prod_{d=1}^D\prod_{v=1}^V \prod_{k=1}^K p(z_{d,v,k} \g \theta_{d,k}, \beta_{v,k})  \prod_{k=1}^K \left[ \prod_{v=1}^V p(\beta_{k,v} \g s_\beta, r_\beta) \prod_{d=1}^D p(\theta_{d,k} \g s_\beta, r_\beta)\right] \\
\log p(\beta_{k,v} \g w, \theta) &\propto \sum_{d=1}^D\sum_{v=1}^V\sum_{k=1}^K \log p(z_{d,v,k} \g \theta_{d,k}, \beta_{v,k}) + \sum_{k=1}^K \left[ \sum_{v=1}^V \log p(\beta_{k,v} \g s_\beta, r_\beta) + \sum_{d=1}^D \log p(\theta_{d,k} \g s_\beta, r_\beta)\right] \\
&\propto \sum_{d=1}^D \log p(z_{d,v,k} \g \theta_{d,k}, \beta_{v,k}) + \log p(\beta_{k,v} \g s_\beta, r_\beta) \\
&= \sum_{d=1}^D \left[ z_{d,v,k}\log(\theta_{d,k}) + z_{d,v,k}\log(\beta_{v,k}) - \theta_{d,k}\beta_{v,k} - \log(z_{d,v,k}!)\right] + s_\beta \log r_\beta  - \log\Gamma(s_\beta) + (s_\beta - 1) \log \beta_{k,v} - r_\beta \beta_{k,v} \\
&\propto \sum_{d=1}^D \left[ z_{d,v,k}\log(\beta_{v,k}) - \theta_{d,k}\beta_{v,k}\right] + (s_\beta - 1) \log \beta_{k,v} - r_\beta \beta_{k,v} \\
&= \log \beta_{k,v} \left[s_\beta - 1 + \sum_{d=1}^D z_{d,v,k} \right] - \beta_{k,v} \left[ r_\beta  + \sum_{d=1}^D \theta_{d,k} \right]\\
&\propto \log\mbox{Gamma}\left(s_\beta + \sum_{d=1}^D z_{d,v,k}, r_\beta  + \sum_{d=1}^D \theta_{d,k}\right) \\
\beta_{k,v} \g \cdots &\sim \mbox{Gamma}\left(s_\beta + \sum_{d=1}^D z_{d,v,k}, r_\beta  + \sum_{d=1}^D \theta_{d,k}\right) \\
\theta_{k,d} \g \cdots &\sim \mbox{Gamma}\left(s_\theta + \sum_{v=1}^V z_{d,v,k}, r_\beta  + \sum_{v=1}^V \beta_{v,k}\right)
\end{align}




\section{Derivation of Hybrid Updates}
\begin{align}
p(\beta_k \g w, \theta, z) &\propto  p(w, \theta, \beta, z) \\
&= \prod_{k=1}^K p(\beta_k \g \alpha_\beta) \prod_{d=1}^D\prod_{v=1}^V p(w_{d,v} \g \theta_d, \beta_v)  \prod_{k=1}^K \prod_{d=1}^D p(\theta_{d,k} \g s_\beta, r_\beta) \\
&= \prod_{k=1}^K p(\beta_k \g \alpha_\beta) \prod_{d=1}^D\prod_{v=1}^V \prod_{k=1}^K p(z_{d,v,k} \g \theta_{d,k}, \beta_{v,k})  \prod_{k=1}^K \prod_{d=1}^D p(\theta_{d,k} \g s_\beta, r_\beta) \\
\log p(\beta_k \g w, \theta, z) &\propto \sum_{k=1}^K \log p(\beta_k \g \alpha_\beta) + \sum_{d=1}^D\sum_{v=1}^V \sum_{k=1}^K \log p(z_{d,v,k} \g \theta_{d,k}, \beta_{v,k}) + \sum_{k=1}^K \sum_{d=1}^D \log p(\theta_{d,k} \g s_\beta, r_\beta) \\
&\propto \log p(\beta_k \g \alpha_\beta) + \sum_{d=1}^D\sum_{v=1}^V \log p(z_{d,v,k} \g \theta_{d,k}, \beta_{v,k}) \\
&= \left[ - \log\mathbf{B}(\alpha_\beta) + \sum_{v=1}^V (\alpha_\beta -1)\log \beta_{k,v}\right] + \sum_{d=1}^D\sum_{v=1}^V \left[ z_{d,v,k} \log\theta_{d,k} + z_{d,v,k} \log\beta_{k,v} - \theta_{d,k}\beta_{k,v} - \log\left(z_{d,v,k}!\right)\right] \\
&\propto \sum_{v=1}^V (\alpha_\beta -1)\log \beta_{k,v} + \sum_{d=1}^D\sum_{v=1}^V \left[ z_{d,v,k} \log\beta_{k,v} - \theta_{d,k}\beta_{k,v} \right] \\
&= \sum_{v=1}^V \log \beta_{k,v} \left[ \alpha_\beta - 1 + \sum_{d=1}^D z_{d,v,k}\right] - \beta_{k,v} \left[\sum_{d=1}^D\theta_{d,k}\right] \\
\beta_{k,v} \g \cdots &= \mbox{Gamma}\left(\alpha_\beta + \sum_{d=1}^D z_{d,v,k}, \sum_{d=1}^D\theta_{d,k}\right)
\end{align}

%TODO:
% go through and check citation styles [ ~\cite{Gusfield:97} and Gusfield~\shortcite{Gusfield:97} ]

TODO: go through and check that indicies are all the same [(d,k) vs (k,d) on theta, for example]


\section{relationship between Dirichlet and Gamma}
we want to show that $z_i \sim \mbox{Gamma}(\alpha_i, \beta)$
$w_i = z_i / sum_n z_n$
$w_i \sim \mbox{Dirichlet}(\alpha_i)$


\begin{align}
p(\sum_i z_i \g \mathbf{\alpha}, \beta) &= \prod_i p(z_i \g \alpha_i, \beta) \\
\log p(\sum_i z_i \g \mathbf{\alpha}, \beta) &= \sum_i \log p(z_i \g \alpha_i, \beta) \\
&= \sum_i \left(\alpha_i \log \beta - \log \Gamma(\alpha_i) + (\alpha_i - 1) s_i - \beta z_i \right)\\
&= \sum_i \left(\alpha_i \log \beta\right) - \sum_i \left( \log \Gamma(\alpha_i)\right) + \sum_i \left((\alpha_i - 1) z_i\right) - \sum_i \left(\beta z_i\right) \\
&= \sum_i \left(\alpha_i \log\right) \beta - \sum_i \left( \log \Gamma(\alpha_i)\right) + \sum_i \left((\alpha_i - 1) z_i\right) - \beta \sum_i z_i \\
\end{align}

\begin{equation}
p(\mathbf{w} \g \mathbf{\alpha}, \beta) = \frac{\prod_i p(z_i \g \alpha_i, \beta)}{p\sum_i \mbox{Gamma}(w_i \g \alpha_i, \beta)}
\end{equation}

\begin{equation}
p(\mathbf{w} \g \mathbf{\alpha}, \beta) = \frac{\prod_i\mbox{Gamma}(z_i \g \alpha_i, \beta)}{\sum_i \mbox{Gamma}(w_i \g \alpha_i, \beta)}
\end{equation}


%\bibliography{library.bib}
%\bibliographystyle{emnlp2016}

\end{document}
